{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv:\n",
    "    def __init__(self, m=3, n=4, gamma=0.9):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = None\n",
    "        self.wall = (m // 2, n // 2 - 1)\n",
    "        self.goal = (0, n - 1)\n",
    "        self.pit = (1, n - 1)\n",
    "        self.gamma = gamma\n",
    "        self.reset()\n",
    "\n",
    "        self.action_map = {\n",
    "            0: \"↑\",\n",
    "            1: \"→\",\n",
    "            2: \"↓\",\n",
    "            3: \"←\",\n",
    "        }\n",
    "        self.policy = None\n",
    "        self.Q = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = [[0] * self.n for _ in range(self.m)]\n",
    "        self.grid = np.array(self.grid).astype(np.float)\n",
    "        self.grid[self.goal[0], self.goal[1]] = 1\n",
    "        self.grid[1, -1] = -1\n",
    "\n",
    "    def render(self, values=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.gcf().canvas.mpl_connect(\n",
    "            \"key_release_event\",\n",
    "            lambda event: [exit(0) if event.key == \"escape\" else None],\n",
    "        )\n",
    "        ax.imshow(self.grid, cmap=cm.get_cmap(\"YlGn\"))\n",
    "        ax.grid(True, which=\"major\", axis=\"both\")\n",
    "        ax.set_xticks(np.arange(-0.5, self.n, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, self.m, 1))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        x_start = -0.5\n",
    "        x_end = self.n\n",
    "        y_start = -0.5\n",
    "        y_end = self.m\n",
    "\n",
    "        jump_x = (x_end - x_start) / (2.0 * self.n)\n",
    "        jump_y = (y_end - y_start) / (2.0 * self.m)\n",
    "        x_positions = np.linspace(start=x_start, stop=x_end, num=self.n, endpoint=False)\n",
    "        y_positions = np.linspace(start=y_start, stop=y_end, num=self.m, endpoint=False)\n",
    "\n",
    "        for y_index, y in enumerate(y_positions):\n",
    "            for x_index, x in enumerate(x_positions):\n",
    "                if self.wall == (x_index, y_index):\n",
    "                    continue\n",
    "                if values is None:\n",
    "                    label = self.action_map[self.policy[y_index, x_index]]\n",
    "                else:\n",
    "                    label = str(round(values[y_index][x_index], 2))\n",
    "                if (y_index, x_index) == self.pit:\n",
    "                    label = -1\n",
    "                if (y_index, x_index) == self.goal:\n",
    "                    label = 1\n",
    "                text_x = x + jump_x * 0.5\n",
    "                text_y = y + jump_y * 0.5\n",
    "                ax.text(text_x, text_y, label, color=\"black\", ha=\"center\", va=\"center\")\n",
    "\n",
    "        plt.pause(0.1)\n",
    "        _ = plt.show()\n",
    "\n",
    "    def step(self, curr_state, action):\n",
    "        assert action <= 3 and action >= 0, \"action out of range\"\n",
    "\n",
    "        m = self.m\n",
    "        n = self.n\n",
    "        rand = np.random.rand()\n",
    "\n",
    "        if action == 0:\n",
    "            if rand <= 0.8:\n",
    "                state = (max(curr_state[0] - 1, 0), curr_state[1])\n",
    "            elif rand > 0.8 and rand <= 0.9:\n",
    "                state = (curr_state[0], max(curr_state[1] - 1, 0))\n",
    "            else:\n",
    "                state = (curr_state[0], min(curr_state[1] + 1, n - 1))\n",
    "\n",
    "        if action == 1:\n",
    "            if rand <= 0.8:\n",
    "                state = (curr_state[0], min(curr_state[1] + 1, n - 1))\n",
    "            elif rand > 0.8 and rand <= 0.9:\n",
    "                state = (max(curr_state[0] - 1, 0), curr_state[1])\n",
    "            else:\n",
    "                state = (min(curr_state[0] + 1, m - 1), curr_state[1])\n",
    "\n",
    "        if action == 2:\n",
    "            if rand <= 0.8:\n",
    "                state = (min(curr_state[0] + 1, m - 1), curr_state[1])\n",
    "            elif rand > 0.8 and rand <= 0.9:\n",
    "                state = (curr_state[0], max(curr_state[1] - 1, 0))\n",
    "            else:\n",
    "                state = (curr_state[0], min(curr_state[1] + 1, n - 1))\n",
    "\n",
    "        if action == 3:\n",
    "            if rand <= 0.8:\n",
    "                state = (curr_state[0], max(curr_state[1] - 1, 0))\n",
    "            elif rand > 0.8 and rand <= 0.9:\n",
    "                state = (max(curr_state[0] - 1, 0), curr_state[1])\n",
    "            else:\n",
    "                state = (min(curr_state[0] + 1, m - 1), curr_state[1])\n",
    "\n",
    "        if state == self.wall:\n",
    "            state = curr_state\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        if state in [self.pit, self.goal]:\n",
    "            done = True\n",
    "            reward = -1 if state == self.pit else 1\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def get_qvalues(self):\n",
    "        Q = {(i, j): np.random.rand(4) for i in range(self.m) for j in range(self.n)}\n",
    "        return Q\n",
    "\n",
    "    def get_policy(self, Q):\n",
    "        policy = [[0] * self.n for _ in range(self.m)]\n",
    "        for k, v in Q.items():\n",
    "            policy[k[0]][k[1]] = np.argmax(v)\n",
    "        return np.array(policy)\n",
    "\n",
    "    def get_action(self, Q, state, eps):\n",
    "        if np.random.rand() <= 1.0 - eps:\n",
    "            return np.argmax(Q[state])\n",
    "        return np.random.randint(4)\n",
    "\n",
    "    def Q_learning(\n",
    "        self, episodes=10, rollout_steps=50, alpha=0.85, eps=0.2, Q=None, render=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Off-Policy TD Control\n",
    "        -> Epsilon greedy policy\n",
    "        \"\"\"\n",
    "        self.Q = Q\n",
    "        if Q is None:\n",
    "            self.Q = self.get_qvalues()\n",
    "\n",
    "        self.policy = self.get_policy(self.Q)\n",
    "        if render:\n",
    "            print(\"Current policy\")\n",
    "            self.render()\n",
    "        \n",
    "        for e in tqdm(range(episodes)):\n",
    "            state = (np.random.randint(self.m), np.random.randint(self.n))\n",
    "            while state in [self.pit, self.wall, self.goal]:\n",
    "                state = (np.random.randint(self.m), np.random.randint(self.n))\n",
    "            action = self.get_action(self.Q, state, eps)\n",
    "            for step in range(rollout_steps):\n",
    "                new_state, reward, done = self.step(state, action)\n",
    "                new_action = np.argmax(self.Q[new_state])\n",
    "                q_update = self.Q[state][action] + alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * self.Q[new_state][new_action]\n",
    "                    - self.Q[state][action]\n",
    "                )\n",
    "                self.Q[state][action] = q_update\n",
    "                state = new_state\n",
    "                action = self.get_action(self.Q, state, eps)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.policy = self.get_policy(self.Q)\n",
    "            if render:\n",
    "                print(\"Episode: {}/{}\".format(e + 1, episodes))\n",
    "                self.render()\n",
    "\n",
    "        return self.policy, self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 9394.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## Start with random Q Values / Policy\n",
    "policy, Q = env.Q_learning(episodes=10000, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Final Policy\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
