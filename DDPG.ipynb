{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQw_TadnOnW6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNXjN6F0P0pk"
   },
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "BATCH_SIZE = 128\n",
    "SEED = 88\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CGXBUuu4P6Fc",
    "outputId": "3476a286-8d2a-4d38-9697-c12afeb44413"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jthmLX2CP7Rb"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3E9dY7EMP-13"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_features, action_size, action_high):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_features, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_size)\n",
    "        self.action_high = action_high\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x)) * self.action_high\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_features, action_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_features + action_size, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIS3PldtQB33"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=100000):\n",
    "        self.state = []\n",
    "        self.action = []\n",
    "        self.next_state = []\n",
    "        self.reward = []\n",
    "        self.done_bool = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done_bool):\n",
    "        if len(self.state) == self.buffer_size:\n",
    "            self.state = self.state[1:]\n",
    "            self.action = self.action[1:]\n",
    "            self.next_state = self.next_state[1:]\n",
    "            self.reward = self.reward[1:]\n",
    "            self.done_bool = self.done_bool[1:]\n",
    "\n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.next_state.append(next_state)\n",
    "        self.reward.append(reward)\n",
    "        self.done_bool.append(done_bool)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        idxs = np.random.randint(low=0, high=len(self.state), size=batch_size)\n",
    "        state = torch.tensor(np.array(self.state)[idxs], dtype=torch.float).squeeze(1)\n",
    "        action = torch.tensor(np.array(self.action)[idxs], dtype=torch.float).squeeze(1)\n",
    "        next_state = torch.tensor(\n",
    "            np.array(self.next_state)[idxs], dtype=torch.float\n",
    "        ).squeeze(1)\n",
    "        reward = torch.tensor(np.array(self.reward)[idxs], dtype=torch.float)\n",
    "        done_bool = torch.tensor(np.array(self.done_bool)[idxs], dtype=torch.float)\n",
    "        return (state, action, next_state, reward, done_bool)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-z4Kag7QGo8"
   },
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        env_name,\n",
    "        device,\n",
    "        gamma=0.99,\n",
    "        tau=0.02,\n",
    "        act_lr=3e-4,\n",
    "        critic_lr=3e-4,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.obs_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.upper_bound = env.action_space.high[0]\n",
    "        self.lower_bound = env.action_space.low[0]\n",
    "\n",
    "        self.actor = Actor(self.obs_size, self.action_size, self.upper_bound).to(device)\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        self.actor_target.eval()\n",
    "\n",
    "        self.critic = Critic(self.obs_size, self.action_size).to(device)\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        self.critic_target.eval()\n",
    "\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=act_lr)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "        self.rewards = []\n",
    "\n",
    "    def critic_criterion(self, pred, target):\n",
    "        return F.smooth_l1_loss(pred, target)\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"actor\": self.actor.state_dict(),\n",
    "                \"actor_target\": self.actor_target.state_dict(),\n",
    "                \"actor_optim\": self.actor_optim.state_dict(),\n",
    "                \"critic\": self.critic.state_dict(),\n",
    "                \"critic_target\": self.critic_target.state_dict(),\n",
    "                \"critic_optim\": self.critic_optim.state_dict(),\n",
    "            },\n",
    "            f\"DDPG_{self.env_name}.pth\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        ckpt = torch.load(path, map_location=self.device)\n",
    "        self.actor.load_state_dict(ckpt[\"actor\"])\n",
    "        self.actor_target.load_state_dict(ckpt[\"actor_target\"])\n",
    "        self.actor_optim.load_state_dict(ckpt[\"actor_optim\"])\n",
    "        self.critic.load_state_dict(ckpt[\"critic\"])\n",
    "        self.critic_target.load_state_dict(ckpt[\"critic_target\"])\n",
    "        self.critic_optim.load_state_dict(ckpt[\"critic_optim\"])\n",
    "        print(\"Checkpoint loaded successfuly\")\n",
    "    \n",
    "    def evaluate_policy(self, env, render=False, create_gif=False):\n",
    "        obs = env.reset()\n",
    "        if create_gif:\n",
    "            import imageio\n",
    "\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            images = [img]\n",
    "            \n",
    "        done = False\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "                action = self.actor(obs).cpu().squeeze().numpy()\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if create_gif:\n",
    "                img = env.render(mode=\"rgb_array\")\n",
    "                images.append(img)\n",
    "\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            rewards += r\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Steps: {steps}, Total reward: {rewards}\")\n",
    "        if create_gif:\n",
    "            imageio.mimsave(\n",
    "                f\"{self.env_name}_agent.gif\",\n",
    "                [np.array(img) for i, img in enumerate(images) if i % 2 == 0],\n",
    "                fps=29,\n",
    "            )\n",
    "            print(\"saved gif\")\n",
    "\n",
    "    def update_target(self):\n",
    "        self.eval()\n",
    "        for param1, param2 in zip(\n",
    "            self.actor.parameters(), self.actor_target.parameters()\n",
    "        ):\n",
    "            param2.data.copy_(self.tau * param1.data + (1 - self.tau) * param2.data)\n",
    "\n",
    "        for param1, param2 in zip(\n",
    "            self.critic.parameters(), self.critic_target.parameters()\n",
    "        ):\n",
    "            param2.data.copy_(self.tau * param1.data + (1 - self.tau) * param2.data)\n",
    "        self.train()\n",
    "\n",
    "    def update(self, iterations, batch_size):\n",
    "        loss_tot = 0.0\n",
    "        for _ in range(iterations):\n",
    "            state, action, next_state, reward, done_bool = self.buffer.sample_batch(\n",
    "                batch_size\n",
    "            )\n",
    "            state = state.to(self.device)\n",
    "            action = action.to(self.device)\n",
    "            next_state = next_state.to(self.device)\n",
    "            reward = reward.unsqueeze(1).to(self.device)\n",
    "            done_bool = done_bool.unsqueeze(1).to(self.device)\n",
    "\n",
    "            Q = self.critic(state, action)\n",
    "\n",
    "            Q_target = self.critic_target(\n",
    "                next_state, self.actor_target(next_state).detach()\n",
    "            )\n",
    "            Q_target = reward + (1 - done_bool) * self.gamma * Q_target.detach()\n",
    "\n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss = self.critic_criterion(Q, Q_target)\n",
    "            critic_loss.backward()\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss = -self.critic(state, self.actor(state))\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optim.step()\n",
    "\n",
    "            self.update_target()\n",
    "\n",
    "            loss_tot += critic_loss.item() + actor_loss.item()\n",
    "        return loss_tot / iterations\n",
    "\n",
    "    def learn(self, env, episodes, batch_size):\n",
    "        writer = SummaryWriter()\n",
    "        steps = 0\n",
    "        for eps in range(episodes):\n",
    "            loss_tracker = AverageMeter()\n",
    "            reward_tracker = AverageMeter()\n",
    "            obs = env.reset()\n",
    "            t = 0\n",
    "            while t < env._max_episode_steps:\n",
    "                if steps < int(1e4):\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = self.actor(\n",
    "                            torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "                        )\n",
    "                    action = action.squeeze(0).cpu().numpy()\n",
    "                    noise = np.random.normal(0, 0.1, size=self.action_size)\n",
    "                    action = (action + noise).clip(self.lower_bound, self.upper_bound)\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                self.buffer.store(obs, action, next_obs, reward, float(done))\n",
    "                reward_tracker.update(reward)\n",
    "                steps += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                obs = next_obs\n",
    "                t += 1\n",
    "\n",
    "            self.rewards.append(reward_tracker.sum)\n",
    "            if len(self.buffer) >= int(1e4):\n",
    "                loss = self.update(iterations=t, batch_size=batch_size)\n",
    "                loss_tracker.update(loss)\n",
    "\n",
    "            writer.add_scalar(\"reward\", reward_tracker.sum, eps + 1)\n",
    "            print(\n",
    "                \"Episode: {}/{}, steps: {}/{}, total steps: {}, loss: {:.2f}, current reward: {:.2f}, running reward: {:.2f}\".format(\n",
    "                    eps,\n",
    "                    episodes,\n",
    "                    t + 1,\n",
    "                    env._max_episode_steps,\n",
    "                    steps,\n",
    "                    loss_tracker.avg,\n",
    "                    reward_tracker.sum,\n",
    "                    np.mean(self.rewards[-100:]),\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.save_checkpoint()\n",
    "\n",
    "            if(np.mean(self.rewards[-100:]) >= 200):\n",
    "                print(\"########## LunarLander Env Solved ###########\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "id": "442la9xGSUj9",
    "outputId": "d6b2d2d3-a5ba-4227-b42a-4945515116ee"
   },
   "outputs": [],
   "source": [
    "## Load tensorboard for visualization of reward\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzVl_dtNQHqF",
    "outputId": "bcd6a0b4-b6e3-4e91-d574-26a329c20ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/1000, steps: 80/1000, total steps: 80, loss: 0.00, current reward: -122.35, running reward: -122.35\n",
      "Episode: 1/1000, steps: 107/1000, total steps: 187, loss: 0.00, current reward: -362.11, running reward: -242.23\n",
      "Episode: 2/1000, steps: 160/1000, total steps: 347, loss: 0.00, current reward: -298.99, running reward: -261.15\n",
      "Episode: 3/1000, steps: 78/1000, total steps: 425, loss: 0.00, current reward: -50.61, running reward: -208.52\n",
      "Episode: 4/1000, steps: 156/1000, total steps: 581, loss: 0.00, current reward: -61.30, running reward: -179.07\n",
      "Episode: 5/1000, steps: 90/1000, total steps: 671, loss: 0.00, current reward: -48.57, running reward: -157.32\n",
      "Episode: 6/1000, steps: 86/1000, total steps: 757, loss: 0.00, current reward: -214.01, running reward: -165.42\n",
      "Episode: 7/1000, steps: 83/1000, total steps: 840, loss: 0.00, current reward: -162.67, running reward: -165.08\n",
      "Episode: 8/1000, steps: 116/1000, total steps: 956, loss: 0.00, current reward: -420.57, running reward: -193.47\n",
      "Episode: 9/1000, steps: 69/1000, total steps: 1025, loss: 0.00, current reward: -83.84, running reward: -182.50\n",
      "Episode: 10/1000, steps: 87/1000, total steps: 1112, loss: 0.00, current reward: -26.47, running reward: -168.32\n",
      "Episode: 11/1000, steps: 90/1000, total steps: 1202, loss: 0.00, current reward: -276.11, running reward: -177.30\n",
      "Episode: 12/1000, steps: 111/1000, total steps: 1313, loss: 0.00, current reward: -488.08, running reward: -201.21\n",
      "Episode: 13/1000, steps: 136/1000, total steps: 1449, loss: 0.00, current reward: -430.62, running reward: -217.59\n",
      "Episode: 14/1000, steps: 92/1000, total steps: 1541, loss: 0.00, current reward: -296.52, running reward: -222.86\n",
      "Episode: 15/1000, steps: 71/1000, total steps: 1612, loss: 0.00, current reward: -78.45, running reward: -213.83\n",
      "Episode: 16/1000, steps: 79/1000, total steps: 1691, loss: 0.00, current reward: -113.42, running reward: -207.92\n",
      "Episode: 17/1000, steps: 131/1000, total steps: 1822, loss: 0.00, current reward: -387.82, running reward: -217.92\n",
      "Episode: 18/1000, steps: 104/1000, total steps: 1926, loss: 0.00, current reward: -267.50, running reward: -220.53\n",
      "Episode: 19/1000, steps: 113/1000, total steps: 2039, loss: 0.00, current reward: -75.05, running reward: -213.25\n",
      "Episode: 20/1000, steps: 123/1000, total steps: 2162, loss: 0.00, current reward: -335.99, running reward: -219.10\n",
      "Episode: 21/1000, steps: 113/1000, total steps: 2275, loss: 0.00, current reward: -285.50, running reward: -222.12\n",
      "Episode: 22/1000, steps: 133/1000, total steps: 2408, loss: 0.00, current reward: -180.50, running reward: -220.31\n",
      "Episode: 23/1000, steps: 104/1000, total steps: 2512, loss: 0.00, current reward: -340.20, running reward: -225.30\n",
      "Episode: 24/1000, steps: 101/1000, total steps: 2613, loss: 0.00, current reward: -271.27, running reward: -227.14\n",
      "Episode: 25/1000, steps: 104/1000, total steps: 2717, loss: 0.00, current reward: -385.30, running reward: -233.22\n",
      "Episode: 26/1000, steps: 111/1000, total steps: 2828, loss: 0.00, current reward: -204.04, running reward: -232.14\n",
      "Episode: 27/1000, steps: 84/1000, total steps: 2912, loss: 0.00, current reward: 35.25, running reward: -222.59\n",
      "Episode: 28/1000, steps: 118/1000, total steps: 3030, loss: 0.00, current reward: -230.17, running reward: -222.86\n",
      "Episode: 29/1000, steps: 67/1000, total steps: 3097, loss: 0.00, current reward: -61.68, running reward: -217.48\n",
      "Episode: 30/1000, steps: 66/1000, total steps: 3163, loss: 0.00, current reward: -78.59, running reward: -213.00\n",
      "Episode: 31/1000, steps: 145/1000, total steps: 3308, loss: 0.00, current reward: -550.19, running reward: -223.54\n",
      "Episode: 32/1000, steps: 89/1000, total steps: 3397, loss: 0.00, current reward: -380.65, running reward: -228.30\n",
      "Episode: 33/1000, steps: 94/1000, total steps: 3491, loss: 0.00, current reward: -204.67, running reward: -227.61\n",
      "Episode: 34/1000, steps: 112/1000, total steps: 3603, loss: 0.00, current reward: -414.92, running reward: -232.96\n",
      "Episode: 35/1000, steps: 119/1000, total steps: 3722, loss: 0.00, current reward: -79.43, running reward: -228.69\n",
      "Episode: 36/1000, steps: 176/1000, total steps: 3898, loss: 0.00, current reward: -163.92, running reward: -226.94\n",
      "Episode: 37/1000, steps: 139/1000, total steps: 4037, loss: 0.00, current reward: -282.16, running reward: -228.40\n",
      "Episode: 38/1000, steps: 146/1000, total steps: 4183, loss: 0.00, current reward: -369.10, running reward: -232.00\n",
      "Episode: 39/1000, steps: 105/1000, total steps: 4288, loss: 0.00, current reward: -316.48, running reward: -234.11\n",
      "Episode: 40/1000, steps: 106/1000, total steps: 4394, loss: 0.00, current reward: -234.44, running reward: -234.12\n",
      "Episode: 41/1000, steps: 117/1000, total steps: 4511, loss: 0.00, current reward: -80.00, running reward: -230.45\n",
      "Episode: 42/1000, steps: 139/1000, total steps: 4650, loss: 0.00, current reward: -242.99, running reward: -230.74\n",
      "Episode: 43/1000, steps: 92/1000, total steps: 4742, loss: 0.00, current reward: -26.79, running reward: -226.11\n",
      "Episode: 44/1000, steps: 89/1000, total steps: 4831, loss: 0.00, current reward: -270.92, running reward: -227.11\n",
      "Episode: 45/1000, steps: 80/1000, total steps: 4911, loss: 0.00, current reward: -114.98, running reward: -224.67\n",
      "Episode: 46/1000, steps: 106/1000, total steps: 5017, loss: 0.00, current reward: -246.60, running reward: -225.13\n",
      "Episode: 47/1000, steps: 107/1000, total steps: 5124, loss: 0.00, current reward: -134.55, running reward: -223.25\n",
      "Episode: 48/1000, steps: 90/1000, total steps: 5214, loss: 0.00, current reward: -205.47, running reward: -222.88\n",
      "Episode: 49/1000, steps: 84/1000, total steps: 5298, loss: 0.00, current reward: -188.67, running reward: -222.20\n",
      "Episode: 50/1000, steps: 87/1000, total steps: 5385, loss: 0.00, current reward: -74.49, running reward: -219.30\n",
      "Episode: 51/1000, steps: 116/1000, total steps: 5501, loss: 0.00, current reward: -172.99, running reward: -218.41\n",
      "Episode: 52/1000, steps: 133/1000, total steps: 5634, loss: 0.00, current reward: -506.86, running reward: -223.86\n",
      "Episode: 53/1000, steps: 105/1000, total steps: 5739, loss: 0.00, current reward: -110.41, running reward: -221.75\n",
      "Episode: 54/1000, steps: 100/1000, total steps: 5839, loss: 0.00, current reward: -156.57, running reward: -220.57\n",
      "Episode: 55/1000, steps: 106/1000, total steps: 5945, loss: 0.00, current reward: -257.17, running reward: -221.22\n",
      "Episode: 56/1000, steps: 84/1000, total steps: 6029, loss: 0.00, current reward: -320.28, running reward: -222.96\n",
      "Episode: 57/1000, steps: 67/1000, total steps: 6096, loss: 0.00, current reward: -89.63, running reward: -220.66\n",
      "Episode: 58/1000, steps: 93/1000, total steps: 6189, loss: 0.00, current reward: -206.44, running reward: -220.42\n",
      "Episode: 59/1000, steps: 132/1000, total steps: 6321, loss: 0.00, current reward: -297.51, running reward: -221.71\n",
      "Episode: 60/1000, steps: 96/1000, total steps: 6417, loss: 0.00, current reward: -410.37, running reward: -224.80\n",
      "Episode: 61/1000, steps: 100/1000, total steps: 6517, loss: 0.00, current reward: -170.57, running reward: -223.92\n",
      "Episode: 62/1000, steps: 94/1000, total steps: 6611, loss: 0.00, current reward: -37.61, running reward: -220.97\n",
      "Episode: 63/1000, steps: 104/1000, total steps: 6715, loss: 0.00, current reward: -458.75, running reward: -224.68\n",
      "Episode: 64/1000, steps: 77/1000, total steps: 6792, loss: 0.00, current reward: -35.84, running reward: -221.78\n",
      "Episode: 65/1000, steps: 108/1000, total steps: 6900, loss: 0.00, current reward: -88.77, running reward: -219.76\n",
      "Episode: 66/1000, steps: 146/1000, total steps: 7046, loss: 0.00, current reward: -103.64, running reward: -218.03\n",
      "Episode: 67/1000, steps: 83/1000, total steps: 7129, loss: 0.00, current reward: -64.34, running reward: -215.77\n",
      "Episode: 68/1000, steps: 187/1000, total steps: 7316, loss: 0.00, current reward: -78.73, running reward: -213.78\n",
      "Episode: 69/1000, steps: 155/1000, total steps: 7471, loss: 0.00, current reward: -106.14, running reward: -212.24\n",
      "Episode: 70/1000, steps: 82/1000, total steps: 7553, loss: 0.00, current reward: -121.20, running reward: -210.96\n",
      "Episode: 71/1000, steps: 164/1000, total steps: 7717, loss: 0.00, current reward: -98.89, running reward: -209.41\n",
      "Episode: 72/1000, steps: 120/1000, total steps: 7837, loss: 0.00, current reward: -52.09, running reward: -207.25\n",
      "Episode: 73/1000, steps: 102/1000, total steps: 7939, loss: 0.00, current reward: -93.76, running reward: -205.72\n",
      "Episode: 74/1000, steps: 99/1000, total steps: 8038, loss: 0.00, current reward: -73.30, running reward: -203.95\n",
      "Episode: 75/1000, steps: 72/1000, total steps: 8110, loss: 0.00, current reward: -22.13, running reward: -201.56\n",
      "Episode: 76/1000, steps: 103/1000, total steps: 8213, loss: 0.00, current reward: -340.67, running reward: -203.37\n",
      "Episode: 77/1000, steps: 137/1000, total steps: 8350, loss: 0.00, current reward: -81.02, running reward: -201.80\n",
      "Episode: 78/1000, steps: 88/1000, total steps: 8438, loss: 0.00, current reward: -363.21, running reward: -203.84\n",
      "Episode: 79/1000, steps: 143/1000, total steps: 8581, loss: 0.00, current reward: -118.78, running reward: -202.78\n",
      "Episode: 80/1000, steps: 76/1000, total steps: 8657, loss: 0.00, current reward: -375.73, running reward: -204.91\n",
      "Episode: 81/1000, steps: 68/1000, total steps: 8725, loss: 0.00, current reward: -74.19, running reward: -203.32\n",
      "Episode: 82/1000, steps: 147/1000, total steps: 8872, loss: 0.00, current reward: -193.26, running reward: -203.20\n",
      "Episode: 83/1000, steps: 170/1000, total steps: 9042, loss: 0.00, current reward: -249.85, running reward: -203.75\n",
      "Episode: 84/1000, steps: 94/1000, total steps: 9136, loss: 0.00, current reward: -74.65, running reward: -202.23\n",
      "Episode: 85/1000, steps: 117/1000, total steps: 9253, loss: 0.00, current reward: -417.33, running reward: -204.73\n",
      "Episode: 86/1000, steps: 95/1000, total steps: 9348, loss: 0.00, current reward: -54.69, running reward: -203.01\n",
      "Episode: 87/1000, steps: 72/1000, total steps: 9420, loss: 0.00, current reward: -106.26, running reward: -201.91\n",
      "Episode: 88/1000, steps: 90/1000, total steps: 9510, loss: 0.00, current reward: -90.75, running reward: -200.66\n",
      "Episode: 89/1000, steps: 105/1000, total steps: 9615, loss: 0.00, current reward: -210.66, running reward: -200.77\n",
      "Episode: 90/1000, steps: 98/1000, total steps: 9713, loss: 0.00, current reward: -96.13, running reward: -199.62\n",
      "Episode: 91/1000, steps: 92/1000, total steps: 9805, loss: 0.00, current reward: -192.94, running reward: -199.55\n",
      "Episode: 92/1000, steps: 125/1000, total steps: 9930, loss: 0.00, current reward: -222.10, running reward: -199.79\n",
      "Episode: 93/1000, steps: 143/1000, total steps: 10073, loss: 2.91, current reward: -504.83, running reward: -203.04\n",
      "Episode: 94/1000, steps: 1000/1000, total steps: 11073, loss: -25.87, current reward: -114.04, running reward: -202.10\n",
      "Episode: 95/1000, steps: 1000/1000, total steps: 12073, loss: -50.11, current reward: 72.05, running reward: -199.25\n",
      "Episode: 96/1000, steps: 1000/1000, total steps: 13073, loss: -53.79, current reward: 44.81, running reward: -196.73\n",
      "Episode: 97/1000, steps: 859/1000, total steps: 13932, loss: -53.83, current reward: 106.56, running reward: -193.63\n",
      "Episode: 98/1000, steps: 304/1000, total steps: 14236, loss: -61.27, current reward: 226.15, running reward: -189.39\n",
      "Episode: 99/1000, steps: 1000/1000, total steps: 15236, loss: -63.57, current reward: 86.86, running reward: -186.63\n",
      "Episode: 100/1000, steps: 635/1000, total steps: 15871, loss: -65.57, current reward: 183.75, running reward: -183.57\n",
      "Episode: 101/1000, steps: 495/1000, total steps: 16366, loss: -67.05, current reward: 286.96, running reward: -177.08\n",
      "Episode: 102/1000, steps: 244/1000, total steps: 16610, loss: -67.84, current reward: 264.21, running reward: -171.45\n",
      "Episode: 103/1000, steps: 718/1000, total steps: 17328, loss: -71.29, current reward: 223.24, running reward: -168.71\n",
      "Episode: 104/1000, steps: 1000/1000, total steps: 18328, loss: -72.35, current reward: 24.54, running reward: -167.85\n",
      "Episode: 105/1000, steps: 509/1000, total steps: 18837, loss: -70.74, current reward: 180.48, running reward: -165.56\n",
      "Episode: 106/1000, steps: 362/1000, total steps: 19199, loss: -70.39, current reward: 242.82, running reward: -160.99\n",
      "Episode: 107/1000, steps: 1000/1000, total steps: 20199, loss: -67.44, current reward: -30.15, running reward: -159.67\n",
      "Episode: 108/1000, steps: 768/1000, total steps: 20967, loss: -65.18, current reward: 147.33, running reward: -153.99\n",
      "Episode: 109/1000, steps: 329/1000, total steps: 21296, loss: -66.28, current reward: 268.80, running reward: -150.46\n",
      "Episode: 110/1000, steps: 865/1000, total steps: 22161, loss: -64.43, current reward: 162.43, running reward: -148.57\n",
      "Episode: 111/1000, steps: 1000/1000, total steps: 23161, loss: -64.17, current reward: -46.49, running reward: -146.28\n",
      "Episode: 112/1000, steps: 1000/1000, total steps: 24161, loss: -63.65, current reward: -17.75, running reward: -141.57\n",
      "Episode: 113/1000, steps: 500/1000, total steps: 24661, loss: -65.22, current reward: 169.87, running reward: -135.57\n",
      "Episode: 114/1000, steps: 1000/1000, total steps: 25661, loss: -62.32, current reward: 19.60, running reward: -132.41\n",
      "Episode: 115/1000, steps: 1000/1000, total steps: 26661, loss: -62.05, current reward: -26.47, running reward: -131.89\n",
      "Episode: 116/1000, steps: 1000/1000, total steps: 27661, loss: -62.32, current reward: 32.01, running reward: -130.43\n",
      "Episode: 117/1000, steps: 1000/1000, total steps: 28661, loss: -62.92, current reward: -35.37, running reward: -126.91\n",
      "Episode: 118/1000, steps: 1000/1000, total steps: 29661, loss: -63.12, current reward: -37.28, running reward: -124.61\n",
      "Episode: 119/1000, steps: 1000/1000, total steps: 30661, loss: -63.90, current reward: -42.88, running reward: -124.28\n",
      "Episode: 120/1000, steps: 1000/1000, total steps: 31661, loss: -65.68, current reward: -42.20, running reward: -121.35\n",
      "Episode: 121/1000, steps: 1000/1000, total steps: 32661, loss: -63.16, current reward: -10.55, running reward: -118.60\n",
      "Episode: 122/1000, steps: 1000/1000, total steps: 33661, loss: -57.24, current reward: -6.25, running reward: -116.85\n",
      "Episode: 123/1000, steps: 1000/1000, total steps: 34661, loss: -50.15, current reward: -40.32, running reward: -113.86\n",
      "Episode: 124/1000, steps: 198/1000, total steps: 34859, loss: -47.91, current reward: -104.76, running reward: -112.19\n",
      "Episode: 125/1000, steps: 194/1000, total steps: 35053, loss: -48.57, current reward: -100.05, running reward: -109.34\n",
      "Episode: 126/1000, steps: 796/1000, total steps: 35849, loss: -48.36, current reward: -234.81, running reward: -109.65\n",
      "Episode: 127/1000, steps: 1000/1000, total steps: 36849, loss: -48.07, current reward: 136.89, running reward: -108.63\n",
      "Episode: 128/1000, steps: 216/1000, total steps: 37065, loss: -49.40, current reward: 263.53, running reward: -103.69\n",
      "Episode: 129/1000, steps: 886/1000, total steps: 37951, loss: -48.42, current reward: 141.58, running reward: -101.66\n",
      "Episode: 130/1000, steps: 1000/1000, total steps: 38951, loss: -50.57, current reward: 154.04, running reward: -99.33\n",
      "Episode: 131/1000, steps: 258/1000, total steps: 39209, loss: -53.43, current reward: 286.99, running reward: -90.96\n",
      "Episode: 132/1000, steps: 768/1000, total steps: 39977, loss: -55.24, current reward: 154.91, running reward: -85.61\n",
      "Episode: 133/1000, steps: 81/1000, total steps: 40058, loss: -56.25, current reward: 44.16, running reward: -83.12\n",
      "Episode: 134/1000, steps: 98/1000, total steps: 40156, loss: -56.06, current reward: 15.59, running reward: -78.81\n",
      "Episode: 135/1000, steps: 939/1000, total steps: 41095, loss: -57.19, current reward: 168.67, running reward: -76.33\n",
      "Episode: 136/1000, steps: 574/1000, total steps: 41669, loss: -57.80, current reward: 151.17, running reward: -73.18\n",
      "Episode: 137/1000, steps: 94/1000, total steps: 41763, loss: -56.47, current reward: 28.45, running reward: -70.07\n",
      "Episode: 138/1000, steps: 265/1000, total steps: 42028, loss: -57.01, current reward: -66.52, running reward: -67.05\n",
      "Episode: 139/1000, steps: 112/1000, total steps: 42140, loss: -56.88, current reward: 16.09, running reward: -63.72\n",
      "Episode: 140/1000, steps: 103/1000, total steps: 42243, loss: -56.41, current reward: 42.31, running reward: -60.96\n",
      "Episode: 141/1000, steps: 96/1000, total steps: 42339, loss: -56.43, current reward: 1.36, running reward: -60.14\n",
      "Episode: 142/1000, steps: 244/1000, total steps: 42583, loss: -56.86, current reward: 276.94, running reward: -54.94\n",
      "Episode: 143/1000, steps: 99/1000, total steps: 42682, loss: -56.39, current reward: 42.63, running reward: -54.25\n",
      "Episode: 144/1000, steps: 268/1000, total steps: 42950, loss: -55.75, current reward: 250.52, running reward: -49.03\n",
      "Episode: 145/1000, steps: 193/1000, total steps: 43143, loss: -55.64, current reward: 287.43, running reward: -45.01\n",
      "Episode: 146/1000, steps: 103/1000, total steps: 43246, loss: -55.82, current reward: 1.23, running reward: -42.53\n",
      "Episode: 147/1000, steps: 105/1000, total steps: 43351, loss: -55.37, current reward: 22.32, running reward: -40.96\n",
      "Episode: 148/1000, steps: 123/1000, total steps: 43474, loss: -55.36, current reward: 50.58, running reward: -38.40\n",
      "Episode: 149/1000, steps: 109/1000, total steps: 43583, loss: -55.56, current reward: -30.78, running reward: -36.82\n",
      "Episode: 150/1000, steps: 124/1000, total steps: 43707, loss: -55.34, current reward: 9.33, running reward: -35.99\n",
      "Episode: 151/1000, steps: 164/1000, total steps: 43871, loss: -55.27, current reward: 237.09, running reward: -31.88\n",
      "Episode: 152/1000, steps: 178/1000, total steps: 44049, loss: -55.60, current reward: 280.66, running reward: -24.01\n",
      "Episode: 153/1000, steps: 426/1000, total steps: 44475, loss: -55.67, current reward: 234.31, running reward: -20.56\n",
      "Episode: 154/1000, steps: 301/1000, total steps: 44776, loss: -56.24, current reward: -122.66, running reward: -20.22\n",
      "Episode: 155/1000, steps: 1000/1000, total steps: 45776, loss: -54.54, current reward: -24.02, running reward: -17.89\n",
      "Episode: 156/1000, steps: 1000/1000, total steps: 46776, loss: -54.76, current reward: 0.92, running reward: -14.68\n",
      "Episode: 157/1000, steps: 128/1000, total steps: 46904, loss: -54.53, current reward: 46.47, running reward: -13.32\n",
      "Episode: 158/1000, steps: 1000/1000, total steps: 47904, loss: -57.09, current reward: 127.20, running reward: -9.98\n",
      "Episode: 159/1000, steps: 379/1000, total steps: 48283, loss: -58.67, current reward: 262.09, running reward: -4.39\n",
      "Episode: 160/1000, steps: 702/1000, total steps: 48985, loss: -59.23, current reward: 86.34, running reward: 0.58\n",
      "Episode: 161/1000, steps: 290/1000, total steps: 49275, loss: -59.94, current reward: 233.66, running reward: 4.62\n",
      "Episode: 162/1000, steps: 304/1000, total steps: 49579, loss: -60.70, current reward: 217.79, running reward: 7.18\n",
      "Episode: 163/1000, steps: 217/1000, total steps: 49796, loss: -60.71, current reward: 255.32, running reward: 14.32\n",
      "Episode: 164/1000, steps: 315/1000, total steps: 50111, loss: -60.76, current reward: 306.59, running reward: 17.74\n",
      "Episode: 165/1000, steps: 247/1000, total steps: 50358, loss: -62.06, current reward: 260.92, running reward: 21.24\n",
      "Episode: 166/1000, steps: 201/1000, total steps: 50559, loss: -62.39, current reward: 313.68, running reward: 25.41\n",
      "Episode: 167/1000, steps: 189/1000, total steps: 50748, loss: -63.26, current reward: 273.30, running reward: 28.79\n",
      "Episode: 168/1000, steps: 210/1000, total steps: 50958, loss: -62.41, current reward: 288.49, running reward: 32.46\n",
      "Episode: 169/1000, steps: 270/1000, total steps: 51228, loss: -62.33, current reward: 272.62, running reward: 36.25\n",
      "Episode: 170/1000, steps: 455/1000, total steps: 51683, loss: -61.36, current reward: 236.97, running reward: 39.83\n",
      "Episode: 171/1000, steps: 251/1000, total steps: 51934, loss: -60.85, current reward: 265.89, running reward: 43.48\n",
      "Episode: 172/1000, steps: 568/1000, total steps: 52502, loss: -60.18, current reward: 253.59, running reward: 46.53\n",
      "Episode: 173/1000, steps: 113/1000, total steps: 52615, loss: -58.04, current reward: 12.73, running reward: 47.60\n",
      "Episode: 174/1000, steps: 186/1000, total steps: 52801, loss: -58.10, current reward: 251.43, running reward: 50.85\n",
      "Episode: 175/1000, steps: 105/1000, total steps: 52906, loss: -58.83, current reward: 13.48, running reward: 51.20\n",
      "Episode: 176/1000, steps: 107/1000, total steps: 53013, loss: -59.37, current reward: 33.12, running reward: 54.94\n",
      "Episode: 177/1000, steps: 195/1000, total steps: 53208, loss: -59.34, current reward: 260.83, running reward: 58.36\n",
      "Episode: 178/1000, steps: 1000/1000, total steps: 54208, loss: -59.34, current reward: 138.18, running reward: 63.37\n",
      "Episode: 179/1000, steps: 111/1000, total steps: 54319, loss: -59.39, current reward: 44.19, running reward: 65.00\n",
      "Episode: 180/1000, steps: 819/1000, total steps: 55138, loss: -57.94, current reward: 244.21, running reward: 71.20\n",
      "Episode: 181/1000, steps: 1000/1000, total steps: 56138, loss: -54.89, current reward: 158.29, running reward: 73.53\n",
      "Episode: 182/1000, steps: 406/1000, total steps: 56544, loss: -55.42, current reward: 196.52, running reward: 77.43\n",
      "Episode: 183/1000, steps: 191/1000, total steps: 56735, loss: -56.99, current reward: 267.29, running reward: 82.60\n",
      "Episode: 184/1000, steps: 603/1000, total steps: 57338, loss: -56.80, current reward: 277.30, running reward: 86.12\n",
      "Episode: 185/1000, steps: 199/1000, total steps: 57537, loss: -56.91, current reward: 243.10, running reward: 92.72\n",
      "Episode: 186/1000, steps: 427/1000, total steps: 57964, loss: -57.80, current reward: 212.96, running reward: 95.40\n",
      "Episode: 187/1000, steps: 202/1000, total steps: 58166, loss: -57.79, current reward: 260.59, running reward: 99.07\n",
      "Episode: 188/1000, steps: 236/1000, total steps: 58402, loss: -57.86, current reward: 300.89, running reward: 102.98\n",
      "Episode: 189/1000, steps: 348/1000, total steps: 58750, loss: -57.81, current reward: 278.98, running reward: 107.88\n",
      "Episode: 190/1000, steps: 226/1000, total steps: 58976, loss: -58.55, current reward: 251.35, running reward: 111.35\n",
      "Episode: 191/1000, steps: 99/1000, total steps: 59075, loss: -59.04, current reward: 8.89, running reward: 113.37\n",
      "Episode: 192/1000, steps: 354/1000, total steps: 59429, loss: -60.04, current reward: 271.67, running reward: 118.31\n",
      "Episode: 193/1000, steps: 146/1000, total steps: 59575, loss: -60.13, current reward: -27.50, running reward: 123.08\n",
      "Episode: 194/1000, steps: 194/1000, total steps: 59769, loss: -59.63, current reward: 290.46, running reward: 127.13\n",
      "Episode: 195/1000, steps: 213/1000, total steps: 59982, loss: -60.21, current reward: 231.11, running reward: 128.72\n",
      "Episode: 196/1000, steps: 143/1000, total steps: 60125, loss: -60.59, current reward: 282.35, running reward: 131.09\n",
      "Episode: 197/1000, steps: 192/1000, total steps: 60317, loss: -59.90, current reward: 276.34, running reward: 132.79\n",
      "Episode: 198/1000, steps: 1000/1000, total steps: 61317, loss: -59.70, current reward: 112.46, running reward: 131.65\n",
      "Episode: 199/1000, steps: 149/1000, total steps: 61466, loss: -60.27, current reward: 227.55, running reward: 133.06\n",
      "Episode: 200/1000, steps: 164/1000, total steps: 61630, loss: -61.59, current reward: 232.48, running reward: 133.55\n",
      "Episode: 201/1000, steps: 1000/1000, total steps: 62630, loss: -63.21, current reward: 124.08, running reward: 131.92\n",
      "Episode: 202/1000, steps: 283/1000, total steps: 62913, loss: -64.15, current reward: 248.36, running reward: 131.76\n",
      "Episode: 203/1000, steps: 202/1000, total steps: 63115, loss: -64.48, current reward: 242.94, running reward: 131.96\n",
      "Episode: 204/1000, steps: 104/1000, total steps: 63219, loss: -65.55, current reward: 8.89, running reward: 131.80\n",
      "Episode: 205/1000, steps: 394/1000, total steps: 63613, loss: -65.81, current reward: -294.11, running reward: 127.06\n",
      "Episode: 206/1000, steps: 346/1000, total steps: 63959, loss: -65.39, current reward: 198.15, running reward: 126.61\n",
      "Episode: 207/1000, steps: 77/1000, total steps: 64036, loss: -65.37, current reward: -19.75, running reward: 126.71\n",
      "Episode: 208/1000, steps: 917/1000, total steps: 64953, loss: -64.10, current reward: 164.50, running reward: 126.88\n",
      "Episode: 209/1000, steps: 439/1000, total steps: 65392, loss: -62.44, current reward: 227.60, running reward: 126.47\n",
      "Episode: 210/1000, steps: 418/1000, total steps: 65810, loss: -62.13, current reward: 170.74, running reward: 126.56\n",
      "Episode: 211/1000, steps: 372/1000, total steps: 66182, loss: -60.70, current reward: 249.79, running reward: 129.52\n",
      "Episode: 212/1000, steps: 186/1000, total steps: 66368, loss: -60.15, current reward: 246.81, running reward: 132.16\n",
      "Episode: 213/1000, steps: 349/1000, total steps: 66717, loss: -60.14, current reward: 268.20, running reward: 133.15\n",
      "Episode: 214/1000, steps: 431/1000, total steps: 67148, loss: -59.48, current reward: 206.20, running reward: 135.01\n",
      "Episode: 215/1000, steps: 225/1000, total steps: 67373, loss: -59.72, current reward: 234.08, running reward: 137.62\n",
      "Episode: 216/1000, steps: 1000/1000, total steps: 68373, loss: -60.11, current reward: 129.51, running reward: 138.59\n",
      "Episode: 217/1000, steps: 757/1000, total steps: 69130, loss: -60.14, current reward: 285.30, running reward: 141.80\n",
      "Episode: 218/1000, steps: 93/1000, total steps: 69223, loss: -57.65, current reward: -1.70, running reward: 142.16\n",
      "Episode: 219/1000, steps: 1000/1000, total steps: 70223, loss: -58.25, current reward: 144.19, running reward: 144.03\n",
      "Episode: 220/1000, steps: 75/1000, total steps: 70298, loss: -57.86, current reward: -29.12, running reward: 144.16\n",
      "Episode: 221/1000, steps: 162/1000, total steps: 70460, loss: -58.62, current reward: 273.01, running reward: 146.99\n",
      "Episode: 222/1000, steps: 616/1000, total steps: 71076, loss: -58.81, current reward: 270.94, running reward: 149.77\n",
      "Episode: 223/1000, steps: 1000/1000, total steps: 72076, loss: -56.94, current reward: 181.37, running reward: 151.98\n",
      "Episode: 224/1000, steps: 295/1000, total steps: 72371, loss: -54.81, current reward: 239.80, running reward: 155.43\n",
      "Episode: 225/1000, steps: 238/1000, total steps: 72609, loss: -53.96, current reward: 286.34, running reward: 159.29\n",
      "Episode: 226/1000, steps: 154/1000, total steps: 72763, loss: -54.60, current reward: 64.32, running reward: 162.28\n",
      "Episode: 227/1000, steps: 152/1000, total steps: 72915, loss: -54.04, current reward: 29.52, running reward: 161.21\n",
      "Episode: 228/1000, steps: 368/1000, total steps: 73283, loss: -53.98, current reward: -61.99, running reward: 157.95\n",
      "Episode: 229/1000, steps: 81/1000, total steps: 73364, loss: -54.17, current reward: -52.52, running reward: 156.01\n",
      "Episode: 230/1000, steps: 72/1000, total steps: 73436, loss: -53.78, current reward: -33.85, running reward: 154.13\n",
      "Episode: 231/1000, steps: 69/1000, total steps: 73505, loss: -55.71, current reward: -42.57, running reward: 150.84\n",
      "Episode: 232/1000, steps: 101/1000, total steps: 73606, loss: -54.86, current reward: -39.82, running reward: 148.89\n",
      "Episode: 233/1000, steps: 99/1000, total steps: 73705, loss: -54.20, current reward: -13.33, running reward: 148.32\n",
      "Episode: 234/1000, steps: 87/1000, total steps: 73792, loss: -55.89, current reward: -10.25, running reward: 148.06\n",
      "Episode: 235/1000, steps: 93/1000, total steps: 73885, loss: -55.69, current reward: -28.54, running reward: 146.09\n",
      "Episode: 236/1000, steps: 219/1000, total steps: 74104, loss: -55.99, current reward: -40.59, running reward: 144.17\n",
      "Episode: 237/1000, steps: 107/1000, total steps: 74211, loss: -56.86, current reward: -45.67, running reward: 143.43\n",
      "Episode: 238/1000, steps: 230/1000, total steps: 74441, loss: -57.41, current reward: 278.76, running reward: 146.88\n",
      "Episode: 239/1000, steps: 1000/1000, total steps: 75441, loss: -55.81, current reward: -49.10, running reward: 146.23\n",
      "Episode: 240/1000, steps: 154/1000, total steps: 75595, loss: -56.41, current reward: 274.59, running reward: 148.55\n",
      "Episode: 241/1000, steps: 84/1000, total steps: 75679, loss: -56.40, current reward: 5.95, running reward: 148.60\n",
      "Episode: 242/1000, steps: 97/1000, total steps: 75776, loss: -56.01, current reward: 48.58, running reward: 146.31\n",
      "Episode: 243/1000, steps: 87/1000, total steps: 75863, loss: -56.17, current reward: 25.14, running reward: 146.14\n",
      "Episode: 244/1000, steps: 80/1000, total steps: 75943, loss: -55.39, current reward: 48.32, running reward: 144.12\n",
      "Episode: 245/1000, steps: 584/1000, total steps: 76527, loss: -55.39, current reward: -62.14, running reward: 140.62\n",
      "Episode: 246/1000, steps: 124/1000, total steps: 76651, loss: -54.95, current reward: 18.57, running reward: 140.79\n",
      "Episode: 247/1000, steps: 149/1000, total steps: 76800, loss: -55.90, current reward: 279.98, running reward: 143.37\n",
      "Episode: 248/1000, steps: 170/1000, total steps: 76970, loss: -55.03, current reward: 273.78, running reward: 145.60\n",
      "Episode: 249/1000, steps: 188/1000, total steps: 77158, loss: -54.74, current reward: 264.27, running reward: 148.55\n",
      "Episode: 250/1000, steps: 246/1000, total steps: 77404, loss: -56.03, current reward: 291.39, running reward: 151.37\n",
      "Episode: 251/1000, steps: 205/1000, total steps: 77609, loss: -56.41, current reward: 252.30, running reward: 151.53\n",
      "Episode: 252/1000, steps: 516/1000, total steps: 78125, loss: -55.33, current reward: 215.73, running reward: 150.88\n",
      "Episode: 253/1000, steps: 156/1000, total steps: 78281, loss: -53.45, current reward: 271.38, running reward: 151.25\n",
      "Episode: 254/1000, steps: 226/1000, total steps: 78507, loss: -53.56, current reward: 293.73, running reward: 155.41\n",
      "Episode: 255/1000, steps: 368/1000, total steps: 78875, loss: -52.39, current reward: 266.08, running reward: 158.31\n",
      "Episode: 256/1000, steps: 275/1000, total steps: 79150, loss: -52.39, current reward: 296.81, running reward: 161.27\n",
      "Episode: 257/1000, steps: 233/1000, total steps: 79383, loss: -52.46, current reward: 268.28, running reward: 163.49\n",
      "Episode: 258/1000, steps: 551/1000, total steps: 79934, loss: -51.71, current reward: 272.43, running reward: 164.94\n",
      "Episode: 259/1000, steps: 293/1000, total steps: 80227, loss: -50.48, current reward: 281.52, running reward: 165.14\n",
      "Episode: 260/1000, steps: 77/1000, total steps: 80304, loss: -50.21, current reward: 24.03, running reward: 164.51\n",
      "Episode: 261/1000, steps: 138/1000, total steps: 80442, loss: -51.24, current reward: 277.51, running reward: 164.95\n",
      "Episode: 262/1000, steps: 136/1000, total steps: 80578, loss: -51.22, current reward: 276.20, running reward: 165.54\n",
      "Episode: 263/1000, steps: 291/1000, total steps: 80869, loss: -51.97, current reward: 264.41, running reward: 165.63\n",
      "Episode: 264/1000, steps: 1000/1000, total steps: 81869, loss: -53.78, current reward: 163.32, running reward: 164.19\n",
      "Episode: 265/1000, steps: 305/1000, total steps: 82174, loss: -56.56, current reward: 299.19, running reward: 164.58\n",
      "Episode: 266/1000, steps: 134/1000, total steps: 82308, loss: -56.35, current reward: 16.67, running reward: 161.61\n",
      "Episode: 267/1000, steps: 176/1000, total steps: 82484, loss: -56.63, current reward: 57.10, running reward: 159.44\n",
      "Episode: 268/1000, steps: 169/1000, total steps: 82653, loss: -56.04, current reward: 2.78, running reward: 156.59\n",
      "Episode: 269/1000, steps: 90/1000, total steps: 82743, loss: -56.38, current reward: -30.83, running reward: 153.55\n",
      "Episode: 270/1000, steps: 110/1000, total steps: 82853, loss: -56.92, current reward: 3.88, running reward: 151.22\n",
      "Episode: 271/1000, steps: 179/1000, total steps: 83032, loss: -56.48, current reward: 40.47, running reward: 148.97\n",
      "Episode: 272/1000, steps: 114/1000, total steps: 83146, loss: -56.40, current reward: -112.83, running reward: 145.30\n",
      "Episode: 273/1000, steps: 117/1000, total steps: 83263, loss: -56.34, current reward: -8.52, running reward: 145.09\n",
      "Episode: 274/1000, steps: 119/1000, total steps: 83382, loss: -58.19, current reward: -24.24, running reward: 142.33\n",
      "Episode: 275/1000, steps: 1000/1000, total steps: 84382, loss: -57.89, current reward: -25.18, running reward: 141.95\n",
      "Episode: 276/1000, steps: 651/1000, total steps: 85033, loss: -57.25, current reward: 194.07, running reward: 143.56\n",
      "Episode: 277/1000, steps: 275/1000, total steps: 85308, loss: -56.65, current reward: 278.47, running reward: 143.73\n",
      "Episode: 278/1000, steps: 288/1000, total steps: 85596, loss: -54.71, current reward: 283.70, running reward: 145.19\n",
      "Episode: 279/1000, steps: 522/1000, total steps: 86118, loss: -53.26, current reward: 249.81, running reward: 147.24\n",
      "Episode: 280/1000, steps: 433/1000, total steps: 86551, loss: -52.56, current reward: 236.93, running reward: 147.17\n",
      "Episode: 281/1000, steps: 1000/1000, total steps: 87551, loss: -49.85, current reward: -6.40, running reward: 145.53\n",
      "Episode: 282/1000, steps: 439/1000, total steps: 87990, loss: -47.79, current reward: 212.78, running reward: 145.69\n",
      "Episode: 283/1000, steps: 283/1000, total steps: 88273, loss: -47.76, current reward: 272.85, running reward: 145.74\n",
      "Episode: 284/1000, steps: 482/1000, total steps: 88755, loss: -47.83, current reward: 218.71, running reward: 145.16\n",
      "Episode: 285/1000, steps: 1000/1000, total steps: 89755, loss: -47.61, current reward: 11.28, running reward: 142.84\n",
      "Episode: 286/1000, steps: 280/1000, total steps: 90035, loss: -47.93, current reward: 298.06, running reward: 143.69\n",
      "Episode: 287/1000, steps: 257/1000, total steps: 90292, loss: -46.88, current reward: 260.25, running reward: 143.69\n",
      "Episode: 288/1000, steps: 1000/1000, total steps: 91292, loss: -46.99, current reward: -35.01, running reward: 140.33\n",
      "Episode: 289/1000, steps: 531/1000, total steps: 91823, loss: -48.01, current reward: 267.23, running reward: 140.21\n",
      "Episode: 290/1000, steps: 795/1000, total steps: 92618, loss: -49.95, current reward: 255.69, running reward: 140.25\n",
      "Episode: 291/1000, steps: 1000/1000, total steps: 93618, loss: -51.22, current reward: 164.71, running reward: 141.81\n",
      "Episode: 292/1000, steps: 315/1000, total steps: 93933, loss: -51.59, current reward: 245.97, running reward: 141.55\n",
      "Episode: 293/1000, steps: 407/1000, total steps: 94340, loss: -51.24, current reward: 268.07, running reward: 144.51\n",
      "Episode: 294/1000, steps: 1000/1000, total steps: 95340, loss: -50.63, current reward: 94.75, running reward: 142.55\n",
      "Episode: 295/1000, steps: 135/1000, total steps: 95475, loss: -50.40, current reward: 261.86, running reward: 142.86\n",
      "Episode: 296/1000, steps: 419/1000, total steps: 95894, loss: -50.30, current reward: 193.47, running reward: 141.97\n",
      "Episode: 297/1000, steps: 65/1000, total steps: 95959, loss: -51.09, current reward: -8.03, running reward: 139.13\n",
      "Episode: 298/1000, steps: 102/1000, total steps: 96061, loss: -50.61, current reward: 52.42, running reward: 138.53\n",
      "Episode: 299/1000, steps: 74/1000, total steps: 96135, loss: -52.24, current reward: 35.08, running reward: 136.60\n",
      "Episode: 300/1000, steps: 80/1000, total steps: 96215, loss: -52.11, current reward: 36.53, running reward: 134.64\n",
      "Episode: 301/1000, steps: 113/1000, total steps: 96328, loss: -50.40, current reward: 21.74, running reward: 133.62\n",
      "Episode: 302/1000, steps: 115/1000, total steps: 96443, loss: -51.42, current reward: 24.97, running reward: 131.39\n",
      "Episode: 303/1000, steps: 75/1000, total steps: 96518, loss: -51.16, current reward: 19.37, running reward: 129.15\n",
      "Episode: 304/1000, steps: 1000/1000, total steps: 97518, loss: -51.32, current reward: -66.19, running reward: 128.40\n",
      "Episode: 305/1000, steps: 788/1000, total steps: 98306, loss: -50.81, current reward: 289.28, running reward: 134.23\n",
      "Episode: 306/1000, steps: 69/1000, total steps: 98375, loss: -50.18, current reward: 16.05, running reward: 132.41\n",
      "Episode: 307/1000, steps: 74/1000, total steps: 98449, loss: -49.88, current reward: 0.20, running reward: 132.61\n",
      "Episode: 308/1000, steps: 76/1000, total steps: 98525, loss: -50.51, current reward: -24.86, running reward: 130.72\n",
      "Episode: 309/1000, steps: 66/1000, total steps: 98591, loss: -52.75, current reward: 19.61, running reward: 128.64\n",
      "Episode: 310/1000, steps: 333/1000, total steps: 98924, loss: -50.97, current reward: 266.28, running reward: 129.59\n",
      "Episode: 311/1000, steps: 308/1000, total steps: 99232, loss: -49.79, current reward: 274.65, running reward: 129.84\n",
      "Episode: 312/1000, steps: 723/1000, total steps: 99955, loss: -48.31, current reward: -43.62, running reward: 126.94\n",
      "Episode: 313/1000, steps: 273/1000, total steps: 100228, loss: -45.93, current reward: 306.39, running reward: 127.32\n",
      "Episode: 314/1000, steps: 110/1000, total steps: 100338, loss: -45.53, current reward: 18.34, running reward: 125.44\n",
      "Episode: 315/1000, steps: 182/1000, total steps: 100520, loss: -46.07, current reward: 305.79, running reward: 126.16\n",
      "Episode: 316/1000, steps: 116/1000, total steps: 100636, loss: -45.08, current reward: 49.08, running reward: 125.36\n",
      "Episode: 317/1000, steps: 189/1000, total steps: 100825, loss: -45.04, current reward: 259.56, running reward: 125.10\n",
      "Episode: 318/1000, steps: 222/1000, total steps: 101047, loss: -42.76, current reward: 79.71, running reward: 125.91\n",
      "Episode: 319/1000, steps: 779/1000, total steps: 101826, loss: -42.30, current reward: -195.26, running reward: 122.52\n",
      "Episode: 320/1000, steps: 164/1000, total steps: 101990, loss: -42.50, current reward: 280.34, running reward: 125.61\n",
      "Episode: 321/1000, steps: 194/1000, total steps: 102184, loss: -42.88, current reward: 236.75, running reward: 125.25\n",
      "Episode: 322/1000, steps: 493/1000, total steps: 102677, loss: -43.77, current reward: -114.79, running reward: 121.39\n",
      "Episode: 323/1000, steps: 112/1000, total steps: 102789, loss: -44.26, current reward: 3.02, running reward: 119.61\n",
      "Episode: 324/1000, steps: 527/1000, total steps: 103316, loss: -45.34, current reward: -181.50, running reward: 115.40\n",
      "Episode: 325/1000, steps: 83/1000, total steps: 103399, loss: -46.15, current reward: 36.98, running reward: 112.90\n",
      "Episode: 326/1000, steps: 311/1000, total steps: 103710, loss: -45.88, current reward: 277.78, running reward: 115.04\n",
      "Episode: 327/1000, steps: 644/1000, total steps: 104354, loss: -44.94, current reward: -149.10, running reward: 113.25\n",
      "Episode: 328/1000, steps: 320/1000, total steps: 104674, loss: -45.31, current reward: 249.53, running reward: 116.37\n",
      "Episode: 329/1000, steps: 235/1000, total steps: 104909, loss: -46.46, current reward: 215.83, running reward: 119.05\n",
      "Episode: 330/1000, steps: 222/1000, total steps: 105131, loss: -47.53, current reward: 284.45, running reward: 122.23\n",
      "Episode: 331/1000, steps: 181/1000, total steps: 105312, loss: -48.37, current reward: 255.46, running reward: 125.21\n",
      "Episode: 332/1000, steps: 144/1000, total steps: 105456, loss: -47.73, current reward: 278.33, running reward: 128.39\n",
      "Episode: 333/1000, steps: 178/1000, total steps: 105634, loss: -48.87, current reward: 262.34, running reward: 131.15\n",
      "Episode: 334/1000, steps: 227/1000, total steps: 105861, loss: -47.13, current reward: 246.32, running reward: 133.72\n",
      "Episode: 335/1000, steps: 234/1000, total steps: 106095, loss: -48.70, current reward: 246.68, running reward: 136.47\n",
      "Episode: 336/1000, steps: 220/1000, total steps: 106315, loss: -47.73, current reward: -50.39, running reward: 136.37\n",
      "Episode: 337/1000, steps: 307/1000, total steps: 106622, loss: -47.17, current reward: -103.28, running reward: 135.79\n",
      "Episode: 338/1000, steps: 1000/1000, total steps: 107622, loss: -45.02, current reward: 134.36, running reward: 134.35\n",
      "Episode: 339/1000, steps: 1000/1000, total steps: 108622, loss: -43.56, current reward: -41.22, running reward: 134.43\n",
      "Episode: 340/1000, steps: 116/1000, total steps: 108738, loss: -41.80, current reward: 4.25, running reward: 131.73\n",
      "Episode: 341/1000, steps: 184/1000, total steps: 108922, loss: -42.38, current reward: 253.36, running reward: 134.20\n",
      "Episode: 342/1000, steps: 150/1000, total steps: 109072, loss: -42.76, current reward: 250.94, running reward: 136.22\n",
      "Episode: 343/1000, steps: 751/1000, total steps: 109823, loss: -42.63, current reward: 257.38, running reward: 138.55\n",
      "Episode: 344/1000, steps: 1000/1000, total steps: 110823, loss: -42.12, current reward: 124.61, running reward: 139.31\n",
      "Episode: 345/1000, steps: 154/1000, total steps: 110977, loss: -43.03, current reward: 266.58, running reward: 142.60\n",
      "Episode: 346/1000, steps: 1000/1000, total steps: 111977, loss: -43.12, current reward: 179.22, running reward: 144.20\n",
      "Episode: 347/1000, steps: 185/1000, total steps: 112162, loss: -43.42, current reward: 282.39, running reward: 144.23\n",
      "Episode: 348/1000, steps: 193/1000, total steps: 112355, loss: -42.82, current reward: 281.76, running reward: 144.31\n",
      "Episode: 349/1000, steps: 157/1000, total steps: 112512, loss: -42.31, current reward: 44.50, running reward: 142.11\n",
      "Episode: 350/1000, steps: 76/1000, total steps: 112588, loss: -43.03, current reward: 52.18, running reward: 139.72\n",
      "Episode: 351/1000, steps: 86/1000, total steps: 112674, loss: -43.76, current reward: 38.48, running reward: 137.58\n",
      "Episode: 352/1000, steps: 80/1000, total steps: 112754, loss: -43.95, current reward: 48.14, running reward: 135.90\n",
      "Episode: 353/1000, steps: 93/1000, total steps: 112847, loss: -43.87, current reward: 13.28, running reward: 133.32\n",
      "Episode: 354/1000, steps: 92/1000, total steps: 112939, loss: -43.84, current reward: 26.40, running reward: 130.65\n",
      "Episode: 355/1000, steps: 71/1000, total steps: 113010, loss: -44.75, current reward: 35.03, running reward: 128.34\n",
      "Episode: 356/1000, steps: 127/1000, total steps: 113137, loss: -45.40, current reward: 49.21, running reward: 125.86\n",
      "Episode: 357/1000, steps: 80/1000, total steps: 113217, loss: -44.77, current reward: 41.03, running reward: 123.59\n",
      "Episode: 358/1000, steps: 102/1000, total steps: 113319, loss: -44.70, current reward: 19.03, running reward: 121.06\n",
      "Episode: 359/1000, steps: 128/1000, total steps: 113447, loss: -44.88, current reward: 18.84, running reward: 118.43\n",
      "Episode: 360/1000, steps: 171/1000, total steps: 113618, loss: -44.04, current reward: 263.69, running reward: 120.83\n",
      "Episode: 361/1000, steps: 222/1000, total steps: 113840, loss: -44.52, current reward: -39.53, running reward: 117.65\n",
      "Episode: 362/1000, steps: 274/1000, total steps: 114114, loss: -45.24, current reward: 254.86, running reward: 117.44\n",
      "Episode: 363/1000, steps: 1000/1000, total steps: 115114, loss: -45.62, current reward: 107.69, running reward: 115.87\n",
      "Episode: 364/1000, steps: 1000/1000, total steps: 116114, loss: -45.87, current reward: 160.87, running reward: 115.85\n",
      "Episode: 365/1000, steps: 192/1000, total steps: 116306, loss: -47.08, current reward: 256.22, running reward: 115.42\n",
      "Episode: 366/1000, steps: 187/1000, total steps: 116493, loss: -46.25, current reward: 299.97, running reward: 118.25\n",
      "Episode: 367/1000, steps: 238/1000, total steps: 116731, loss: -46.22, current reward: 265.40, running reward: 120.34\n",
      "Episode: 368/1000, steps: 193/1000, total steps: 116924, loss: -46.94, current reward: 285.86, running reward: 123.17\n",
      "Episode: 369/1000, steps: 128/1000, total steps: 117052, loss: -47.26, current reward: 280.97, running reward: 126.28\n",
      "Episode: 370/1000, steps: 1000/1000, total steps: 118052, loss: -46.27, current reward: 109.41, running reward: 127.34\n",
      "Episode: 371/1000, steps: 1000/1000, total steps: 119052, loss: -45.91, current reward: -52.75, running reward: 126.41\n",
      "Episode: 372/1000, steps: 269/1000, total steps: 119321, loss: -46.02, current reward: 273.42, running reward: 130.27\n",
      "Episode: 373/1000, steps: 355/1000, total steps: 119676, loss: -48.24, current reward: 308.88, running reward: 133.44\n",
      "Episode: 374/1000, steps: 195/1000, total steps: 119871, loss: -50.38, current reward: 258.46, running reward: 136.27\n",
      "Episode: 375/1000, steps: 258/1000, total steps: 120129, loss: -51.71, current reward: 264.48, running reward: 139.17\n",
      "Episode: 376/1000, steps: 395/1000, total steps: 120524, loss: -53.42, current reward: 290.81, running reward: 140.14\n",
      "Episode: 377/1000, steps: 513/1000, total steps: 121037, loss: -54.22, current reward: -75.24, running reward: 136.60\n",
      "Episode: 378/1000, steps: 336/1000, total steps: 121373, loss: -52.26, current reward: 229.02, running reward: 136.05\n",
      "Episode: 379/1000, steps: 278/1000, total steps: 121651, loss: -50.18, current reward: 264.37, running reward: 136.20\n",
      "Episode: 380/1000, steps: 457/1000, total steps: 122108, loss: -49.13, current reward: 209.09, running reward: 135.92\n",
      "Episode: 381/1000, steps: 321/1000, total steps: 122429, loss: -48.17, current reward: 240.99, running reward: 138.39\n",
      "Episode: 382/1000, steps: 1000/1000, total steps: 123429, loss: -46.04, current reward: -20.64, running reward: 136.06\n",
      "Episode: 383/1000, steps: 1000/1000, total steps: 124429, loss: -43.19, current reward: 12.41, running reward: 133.45\n",
      "Episode: 384/1000, steps: 143/1000, total steps: 124572, loss: -40.88, current reward: 256.33, running reward: 133.83\n",
      "Episode: 385/1000, steps: 258/1000, total steps: 124830, loss: -40.76, current reward: 300.23, running reward: 136.72\n",
      "Episode: 386/1000, steps: 197/1000, total steps: 125027, loss: -41.22, current reward: 296.44, running reward: 136.70\n",
      "Episode: 387/1000, steps: 720/1000, total steps: 125747, loss: -39.72, current reward: 189.67, running reward: 136.00\n",
      "Episode: 388/1000, steps: 415/1000, total steps: 126162, loss: -39.34, current reward: 265.96, running reward: 139.01\n",
      "Episode: 389/1000, steps: 84/1000, total steps: 126246, loss: -38.59, current reward: -2.81, running reward: 136.31\n",
      "Episode: 390/1000, steps: 1000/1000, total steps: 127246, loss: -38.14, current reward: 43.48, running reward: 134.18\n",
      "Episode: 391/1000, steps: 243/1000, total steps: 127489, loss: -36.86, current reward: 290.56, running reward: 135.44\n",
      "Episode: 392/1000, steps: 1000/1000, total steps: 128489, loss: -38.00, current reward: 170.49, running reward: 134.69\n",
      "Episode: 393/1000, steps: 198/1000, total steps: 128687, loss: -38.65, current reward: 10.39, running reward: 132.11\n",
      "Episode: 394/1000, steps: 242/1000, total steps: 128929, loss: -37.85, current reward: 282.25, running reward: 133.99\n",
      "Episode: 395/1000, steps: 183/1000, total steps: 129112, loss: -39.09, current reward: 252.55, running reward: 133.89\n",
      "Episode: 396/1000, steps: 279/1000, total steps: 129391, loss: -38.44, current reward: 260.09, running reward: 134.56\n",
      "Episode: 397/1000, steps: 174/1000, total steps: 129565, loss: -39.39, current reward: 21.45, running reward: 134.85\n",
      "Episode: 398/1000, steps: 113/1000, total steps: 129678, loss: -38.43, current reward: 59.33, running reward: 134.92\n",
      "Episode: 399/1000, steps: 95/1000, total steps: 129773, loss: -40.17, current reward: -0.38, running reward: 134.57\n",
      "Episode: 400/1000, steps: 297/1000, total steps: 130070, loss: -40.15, current reward: 265.39, running reward: 136.86\n",
      "Episode: 401/1000, steps: 644/1000, total steps: 130714, loss: -41.29, current reward: 192.74, running reward: 138.57\n",
      "Episode: 402/1000, steps: 541/1000, total steps: 131255, loss: -43.12, current reward: 266.70, running reward: 140.98\n",
      "Episode: 403/1000, steps: 169/1000, total steps: 131424, loss: -43.35, current reward: 252.22, running reward: 143.31\n",
      "Episode: 404/1000, steps: 190/1000, total steps: 131614, loss: -44.47, current reward: 10.33, running reward: 144.08\n",
      "Episode: 405/1000, steps: 291/1000, total steps: 131905, loss: -44.41, current reward: 276.74, running reward: 143.95\n",
      "Episode: 406/1000, steps: 260/1000, total steps: 132165, loss: -45.26, current reward: 264.28, running reward: 146.44\n",
      "Episode: 407/1000, steps: 73/1000, total steps: 132238, loss: -47.11, current reward: 48.09, running reward: 146.91\n",
      "Episode: 408/1000, steps: 89/1000, total steps: 132327, loss: -46.88, current reward: -2.57, running reward: 147.14\n",
      "Episode: 409/1000, steps: 104/1000, total steps: 132431, loss: -45.54, current reward: -7.58, running reward: 146.87\n",
      "Episode: 410/1000, steps: 69/1000, total steps: 132500, loss: -47.12, current reward: -11.07, running reward: 144.09\n",
      "Episode: 411/1000, steps: 1000/1000, total steps: 133500, loss: -48.06, current reward: 158.69, running reward: 142.93\n",
      "Episode: 412/1000, steps: 290/1000, total steps: 133790, loss: -51.13, current reward: 272.27, running reward: 146.09\n",
      "Episode: 413/1000, steps: 79/1000, total steps: 133869, loss: -52.14, current reward: 40.57, running reward: 143.43\n",
      "Episode: 414/1000, steps: 947/1000, total steps: 134816, loss: -51.00, current reward: 260.75, running reward: 145.86\n",
      "Episode: 415/1000, steps: 163/1000, total steps: 134979, loss: -49.56, current reward: 276.28, running reward: 145.56\n",
      "Episode: 416/1000, steps: 81/1000, total steps: 135060, loss: -51.97, current reward: -4.80, running reward: 145.02\n",
      "Episode: 417/1000, steps: 96/1000, total steps: 135156, loss: -49.29, current reward: -72.28, running reward: 141.70\n",
      "Episode: 418/1000, steps: 88/1000, total steps: 135244, loss: -49.03, current reward: 17.80, running reward: 141.09\n",
      "Episode: 419/1000, steps: 1000/1000, total steps: 136244, loss: -48.32, current reward: 154.91, running reward: 144.59\n",
      "Episode: 420/1000, steps: 1000/1000, total steps: 137244, loss: -43.81, current reward: 122.48, running reward: 143.01\n",
      "Episode: 421/1000, steps: 94/1000, total steps: 137338, loss: -44.54, current reward: -0.24, running reward: 140.64\n",
      "Episode: 422/1000, steps: 1000/1000, total steps: 138338, loss: -46.03, current reward: 57.18, running reward: 142.36\n",
      "Episode: 423/1000, steps: 1000/1000, total steps: 139338, loss: -45.29, current reward: 26.72, running reward: 142.60\n",
      "Episode: 424/1000, steps: 1000/1000, total steps: 140338, loss: -43.97, current reward: 162.96, running reward: 146.04\n",
      "Episode: 425/1000, steps: 512/1000, total steps: 140850, loss: -43.85, current reward: 120.79, running reward: 146.88\n",
      "Episode: 426/1000, steps: 247/1000, total steps: 141097, loss: -42.91, current reward: 241.03, running reward: 146.51\n",
      "Episode: 427/1000, steps: 175/1000, total steps: 141272, loss: -41.81, current reward: 275.23, running reward: 150.75\n",
      "Episode: 428/1000, steps: 365/1000, total steps: 141637, loss: -43.93, current reward: 250.16, running reward: 150.76\n",
      "Episode: 429/1000, steps: 434/1000, total steps: 142071, loss: -43.64, current reward: 222.93, running reward: 150.83\n",
      "Episode: 430/1000, steps: 186/1000, total steps: 142257, loss: -43.40, current reward: 274.59, running reward: 150.73\n",
      "Episode: 431/1000, steps: 497/1000, total steps: 142754, loss: -45.99, current reward: -33.21, running reward: 147.85\n",
      "Episode: 432/1000, steps: 1000/1000, total steps: 143754, loss: -45.34, current reward: -23.80, running reward: 144.82\n",
      "Episode: 433/1000, steps: 368/1000, total steps: 144122, loss: -46.44, current reward: -196.15, running reward: 140.24\n",
      "Episode: 434/1000, steps: 288/1000, total steps: 144410, loss: -44.65, current reward: 227.05, running reward: 140.05\n",
      "Episode: 435/1000, steps: 207/1000, total steps: 144617, loss: -47.03, current reward: 265.70, running reward: 140.24\n",
      "Episode: 436/1000, steps: 197/1000, total steps: 144814, loss: -46.11, current reward: 284.06, running reward: 143.58\n",
      "Episode: 437/1000, steps: 230/1000, total steps: 145044, loss: -43.59, current reward: 258.95, running reward: 147.20\n",
      "Episode: 438/1000, steps: 327/1000, total steps: 145371, loss: -45.30, current reward: 252.67, running reward: 148.39\n",
      "Episode: 439/1000, steps: 115/1000, total steps: 145486, loss: -43.22, current reward: 47.77, running reward: 149.28\n",
      "Episode: 440/1000, steps: 242/1000, total steps: 145728, loss: -45.11, current reward: 276.56, running reward: 152.00\n",
      "Episode: 441/1000, steps: 109/1000, total steps: 145837, loss: -43.47, current reward: 2.08, running reward: 149.49\n",
      "Episode: 442/1000, steps: 329/1000, total steps: 146166, loss: -48.42, current reward: 270.97, running reward: 149.69\n",
      "Episode: 443/1000, steps: 81/1000, total steps: 146247, loss: -41.45, current reward: 0.33, running reward: 147.12\n",
      "Episode: 444/1000, steps: 467/1000, total steps: 146714, loss: -42.21, current reward: 271.89, running reward: 148.59\n",
      "Episode: 445/1000, steps: 212/1000, total steps: 146926, loss: -46.69, current reward: 253.78, running reward: 148.46\n",
      "Episode: 446/1000, steps: 884/1000, total steps: 147810, loss: -44.71, current reward: 250.59, running reward: 149.18\n",
      "Episode: 447/1000, steps: 344/1000, total steps: 148154, loss: -45.94, current reward: 214.15, running reward: 148.49\n",
      "Episode: 448/1000, steps: 165/1000, total steps: 148319, loss: -45.97, current reward: 263.75, running reward: 148.31\n",
      "Episode: 449/1000, steps: 85/1000, total steps: 148404, loss: -46.78, current reward: 19.11, running reward: 148.06\n",
      "Episode: 450/1000, steps: 163/1000, total steps: 148567, loss: -46.67, current reward: 272.63, running reward: 150.26\n",
      "Episode: 451/1000, steps: 890/1000, total steps: 149457, loss: -45.09, current reward: -165.50, running reward: 148.22\n",
      "Episode: 452/1000, steps: 1000/1000, total steps: 150457, loss: -45.32, current reward: 159.47, running reward: 149.34\n",
      "Episode: 453/1000, steps: 153/1000, total steps: 150610, loss: -45.49, current reward: 269.39, running reward: 151.90\n",
      "Episode: 454/1000, steps: 300/1000, total steps: 150910, loss: -42.73, current reward: 270.08, running reward: 154.34\n",
      "Episode: 455/1000, steps: 76/1000, total steps: 150986, loss: -44.88, current reward: 47.66, running reward: 154.46\n",
      "Episode: 456/1000, steps: 84/1000, total steps: 151070, loss: -46.19, current reward: 32.88, running reward: 154.30\n",
      "Episode: 457/1000, steps: 182/1000, total steps: 151252, loss: -44.27, current reward: 243.06, running reward: 156.32\n",
      "Episode: 458/1000, steps: 1000/1000, total steps: 152252, loss: -44.34, current reward: 108.01, running reward: 157.21\n",
      "Episode: 459/1000, steps: 192/1000, total steps: 152444, loss: -48.15, current reward: 280.74, running reward: 159.83\n",
      "Episode: 460/1000, steps: 164/1000, total steps: 152608, loss: -46.65, current reward: 258.58, running reward: 159.78\n",
      "Episode: 461/1000, steps: 174/1000, total steps: 152782, loss: -47.03, current reward: 300.71, running reward: 163.18\n",
      "Episode: 462/1000, steps: 169/1000, total steps: 152951, loss: -45.42, current reward: 275.03, running reward: 163.38\n",
      "Episode: 463/1000, steps: 147/1000, total steps: 153098, loss: -45.09, current reward: 243.90, running reward: 164.74\n",
      "Episode: 464/1000, steps: 163/1000, total steps: 153261, loss: -40.09, current reward: 253.07, running reward: 165.66\n",
      "Episode: 465/1000, steps: 91/1000, total steps: 153352, loss: -45.32, current reward: 38.35, running reward: 163.49\n",
      "Episode: 466/1000, steps: 106/1000, total steps: 153458, loss: -44.66, current reward: 33.82, running reward: 160.82\n",
      "Episode: 467/1000, steps: 139/1000, total steps: 153597, loss: -43.85, current reward: 262.03, running reward: 160.79\n",
      "Episode: 468/1000, steps: 175/1000, total steps: 153772, loss: -45.22, current reward: 276.18, running reward: 160.69\n",
      "Episode: 469/1000, steps: 100/1000, total steps: 153872, loss: -44.58, current reward: 45.83, running reward: 158.34\n",
      "Episode: 470/1000, steps: 157/1000, total steps: 154029, loss: -44.82, current reward: 253.66, running reward: 159.79\n",
      "Episode: 471/1000, steps: 154/1000, total steps: 154183, loss: -45.35, current reward: 285.30, running reward: 163.17\n",
      "Episode: 472/1000, steps: 162/1000, total steps: 154345, loss: -42.49, current reward: 275.38, running reward: 163.19\n",
      "Episode: 473/1000, steps: 177/1000, total steps: 154522, loss: -45.51, current reward: 289.94, running reward: 163.00\n",
      "Episode: 474/1000, steps: 170/1000, total steps: 154692, loss: -44.93, current reward: 279.92, running reward: 163.21\n",
      "Episode: 475/1000, steps: 211/1000, total steps: 154903, loss: -45.51, current reward: 265.37, running reward: 163.22\n",
      "Episode: 476/1000, steps: 162/1000, total steps: 155065, loss: -47.24, current reward: 276.50, running reward: 163.08\n",
      "Episode: 477/1000, steps: 364/1000, total steps: 155429, loss: -46.72, current reward: 304.62, running reward: 166.87\n",
      "Episode: 478/1000, steps: 551/1000, total steps: 155980, loss: -46.28, current reward: 293.45, running reward: 167.52\n",
      "Episode: 479/1000, steps: 251/1000, total steps: 156231, loss: -47.94, current reward: 245.18, running reward: 167.33\n",
      "Episode: 480/1000, steps: 92/1000, total steps: 156323, loss: -42.96, current reward: 8.46, running reward: 165.32\n",
      "Episode: 481/1000, steps: 74/1000, total steps: 156397, loss: -48.80, current reward: 39.30, running reward: 163.30\n",
      "Episode: 482/1000, steps: 295/1000, total steps: 156692, loss: -46.31, current reward: 254.44, running reward: 166.05\n",
      "Episode: 483/1000, steps: 1000/1000, total steps: 157692, loss: -43.75, current reward: 123.63, running reward: 167.17\n",
      "Episode: 484/1000, steps: 766/1000, total steps: 158458, loss: -44.89, current reward: 261.39, running reward: 167.22\n",
      "Episode: 485/1000, steps: 268/1000, total steps: 158726, loss: -46.70, current reward: 268.85, running reward: 166.90\n",
      "Episode: 486/1000, steps: 109/1000, total steps: 158835, loss: -48.48, current reward: 17.59, running reward: 164.11\n",
      "Episode: 487/1000, steps: 109/1000, total steps: 158944, loss: -49.67, current reward: 12.84, running reward: 162.35\n",
      "Episode: 488/1000, steps: 276/1000, total steps: 159220, loss: -50.53, current reward: 286.57, running reward: 162.55\n",
      "Episode: 489/1000, steps: 77/1000, total steps: 159297, loss: -44.67, current reward: -40.25, running reward: 162.18\n",
      "Episode: 490/1000, steps: 99/1000, total steps: 159396, loss: -51.16, current reward: -32.31, running reward: 161.42\n",
      "Episode: 491/1000, steps: 836/1000, total steps: 160232, loss: -51.06, current reward: 263.15, running reward: 161.15\n",
      "Episode: 492/1000, steps: 578/1000, total steps: 160810, loss: -52.63, current reward: 265.12, running reward: 162.09\n",
      "Episode: 493/1000, steps: 252/1000, total steps: 161062, loss: -54.86, current reward: 21.94, running reward: 162.21\n",
      "Episode: 494/1000, steps: 85/1000, total steps: 161147, loss: -55.20, current reward: 50.25, running reward: 159.89\n",
      "Episode: 495/1000, steps: 1000/1000, total steps: 162147, loss: -54.82, current reward: 89.63, running reward: 158.26\n",
      "Episode: 496/1000, steps: 139/1000, total steps: 162286, loss: -55.87, current reward: 50.11, running reward: 156.16\n",
      "Episode: 497/1000, steps: 177/1000, total steps: 162463, loss: -52.87, current reward: -272.45, running reward: 153.22\n",
      "Episode: 498/1000, steps: 332/1000, total steps: 162795, loss: -54.40, current reward: -246.78, running reward: 150.16\n",
      "Episode: 499/1000, steps: 322/1000, total steps: 163117, loss: -56.42, current reward: -94.08, running reward: 149.22\n",
      "Episode: 500/1000, steps: 875/1000, total steps: 163992, loss: -55.54, current reward: 236.01, running reward: 148.93\n",
      "Episode: 501/1000, steps: 267/1000, total steps: 164259, loss: -54.20, current reward: 268.36, running reward: 149.68\n",
      "Episode: 502/1000, steps: 168/1000, total steps: 164427, loss: -53.93, current reward: 245.10, running reward: 149.47\n",
      "Episode: 503/1000, steps: 762/1000, total steps: 165189, loss: -51.36, current reward: 237.90, running reward: 149.33\n",
      "Episode: 504/1000, steps: 205/1000, total steps: 165394, loss: -51.39, current reward: -277.00, running reward: 146.45\n",
      "Episode: 505/1000, steps: 209/1000, total steps: 165603, loss: -49.91, current reward: 301.93, running reward: 146.70\n",
      "Episode: 506/1000, steps: 188/1000, total steps: 165791, loss: -48.25, current reward: 273.00, running reward: 146.79\n",
      "Episode: 507/1000, steps: 138/1000, total steps: 165929, loss: -45.94, current reward: 49.25, running reward: 146.80\n",
      "Episode: 508/1000, steps: 213/1000, total steps: 166142, loss: -47.10, current reward: 262.70, running reward: 149.46\n",
      "Episode: 509/1000, steps: 203/1000, total steps: 166345, loss: -45.31, current reward: -70.47, running reward: 148.83\n",
      "Episode: 510/1000, steps: 426/1000, total steps: 166771, loss: -41.34, current reward: 269.33, running reward: 151.63\n",
      "Episode: 511/1000, steps: 384/1000, total steps: 167155, loss: -44.95, current reward: 233.82, running reward: 152.38\n",
      "Episode: 512/1000, steps: 197/1000, total steps: 167352, loss: -45.63, current reward: 267.71, running reward: 152.34\n",
      "Episode: 513/1000, steps: 168/1000, total steps: 167520, loss: -41.67, current reward: 0.45, running reward: 151.93\n",
      "Episode: 514/1000, steps: 523/1000, total steps: 168043, loss: -44.10, current reward: 195.78, running reward: 151.28\n",
      "Episode: 515/1000, steps: 218/1000, total steps: 168261, loss: -44.87, current reward: 273.46, running reward: 151.26\n",
      "Episode: 516/1000, steps: 267/1000, total steps: 168528, loss: -45.13, current reward: -165.36, running reward: 149.65\n",
      "Episode: 517/1000, steps: 272/1000, total steps: 168800, loss: -43.21, current reward: 229.11, running reward: 152.66\n",
      "Episode: 518/1000, steps: 216/1000, total steps: 169016, loss: -47.21, current reward: 259.72, running reward: 155.08\n",
      "Episode: 519/1000, steps: 1000/1000, total steps: 170016, loss: -43.80, current reward: -40.82, running reward: 153.13\n",
      "Episode: 520/1000, steps: 1000/1000, total steps: 171016, loss: -41.86, current reward: -11.01, running reward: 151.79\n",
      "Episode: 521/1000, steps: 154/1000, total steps: 171170, loss: -44.07, current reward: 279.40, running reward: 154.59\n",
      "Episode: 522/1000, steps: 758/1000, total steps: 171928, loss: -42.12, current reward: 237.62, running reward: 156.39\n",
      "Episode: 523/1000, steps: 162/1000, total steps: 172090, loss: -47.69, current reward: 273.75, running reward: 158.86\n",
      "Episode: 524/1000, steps: 248/1000, total steps: 172338, loss: -48.42, current reward: 278.40, running reward: 160.02\n",
      "Episode: 525/1000, steps: 434/1000, total steps: 172772, loss: -49.63, current reward: 272.05, running reward: 161.53\n",
      "Episode: 526/1000, steps: 105/1000, total steps: 172877, loss: -52.02, current reward: -2.90, running reward: 159.09\n",
      "Episode: 527/1000, steps: 78/1000, total steps: 172955, loss: -52.71, current reward: 16.66, running reward: 156.51\n",
      "Episode: 528/1000, steps: 168/1000, total steps: 173123, loss: -46.79, current reward: 230.69, running reward: 156.31\n",
      "Episode: 529/1000, steps: 168/1000, total steps: 173291, loss: -47.68, current reward: 270.22, running reward: 156.78\n",
      "Episode: 530/1000, steps: 254/1000, total steps: 173545, loss: -54.09, current reward: 192.34, running reward: 155.96\n",
      "Episode: 531/1000, steps: 1000/1000, total steps: 174545, loss: -51.52, current reward: 123.96, running reward: 157.53\n",
      "Episode: 532/1000, steps: 1000/1000, total steps: 175545, loss: -49.45, current reward: -56.20, running reward: 157.21\n",
      "Episode: 533/1000, steps: 209/1000, total steps: 175754, loss: -48.57, current reward: 262.35, running reward: 161.79\n",
      "Episode: 534/1000, steps: 1000/1000, total steps: 176754, loss: -48.76, current reward: -58.91, running reward: 158.93\n",
      "Episode: 535/1000, steps: 456/1000, total steps: 177210, loss: -48.63, current reward: 275.68, running reward: 159.03\n",
      "Episode: 536/1000, steps: 90/1000, total steps: 177300, loss: -44.69, current reward: 44.76, running reward: 156.64\n",
      "Episode: 537/1000, steps: 189/1000, total steps: 177489, loss: -50.37, current reward: 240.17, running reward: 156.45\n",
      "Episode: 538/1000, steps: 249/1000, total steps: 177738, loss: -48.92, current reward: 279.93, running reward: 156.73\n",
      "Episode: 539/1000, steps: 164/1000, total steps: 177902, loss: -50.20, current reward: 232.56, running reward: 158.57\n",
      "Episode: 540/1000, steps: 292/1000, total steps: 178194, loss: -48.70, current reward: 235.41, running reward: 158.16\n",
      "Episode: 541/1000, steps: 249/1000, total steps: 178443, loss: -46.19, current reward: 288.14, running reward: 161.02\n",
      "Episode: 542/1000, steps: 195/1000, total steps: 178638, loss: -44.44, current reward: 284.23, running reward: 161.16\n",
      "Episode: 543/1000, steps: 185/1000, total steps: 178823, loss: -45.23, current reward: 292.80, running reward: 164.08\n",
      "Episode: 544/1000, steps: 168/1000, total steps: 178991, loss: -49.00, current reward: 256.49, running reward: 163.93\n",
      "Episode: 545/1000, steps: 187/1000, total steps: 179178, loss: -45.13, current reward: 280.28, running reward: 164.19\n",
      "Episode: 546/1000, steps: 181/1000, total steps: 179359, loss: -43.64, current reward: 275.14, running reward: 164.44\n",
      "Episode: 547/1000, steps: 161/1000, total steps: 179520, loss: -47.51, current reward: 271.81, running reward: 165.01\n",
      "Episode: 548/1000, steps: 161/1000, total steps: 179681, loss: -42.84, current reward: 258.49, running reward: 164.96\n",
      "Episode: 549/1000, steps: 178/1000, total steps: 179859, loss: -48.09, current reward: 255.67, running reward: 167.33\n",
      "Episode: 550/1000, steps: 300/1000, total steps: 180159, loss: -45.17, current reward: 286.03, running reward: 167.46\n",
      "Episode: 551/1000, steps: 520/1000, total steps: 180679, loss: -48.29, current reward: 270.73, running reward: 171.82\n",
      "Episode: 552/1000, steps: 207/1000, total steps: 180886, loss: -48.57, current reward: 276.50, running reward: 172.99\n",
      "Episode: 553/1000, steps: 148/1000, total steps: 181034, loss: -47.90, current reward: 52.45, running reward: 170.82\n",
      "Episode: 554/1000, steps: 232/1000, total steps: 181266, loss: -48.64, current reward: 284.72, running reward: 170.97\n",
      "Episode: 555/1000, steps: 293/1000, total steps: 181559, loss: -51.15, current reward: 254.17, running reward: 173.03\n",
      "Episode: 556/1000, steps: 246/1000, total steps: 181805, loss: -51.65, current reward: 292.43, running reward: 175.63\n",
      "Episode: 557/1000, steps: 209/1000, total steps: 182014, loss: -52.05, current reward: 290.42, running reward: 176.10\n",
      "Episode: 558/1000, steps: 182/1000, total steps: 182196, loss: -45.83, current reward: 271.42, running reward: 177.74\n",
      "Episode: 559/1000, steps: 257/1000, total steps: 182453, loss: -52.78, current reward: 271.22, running reward: 177.64\n",
      "Episode: 560/1000, steps: 1000/1000, total steps: 183453, loss: -51.70, current reward: 179.30, running reward: 176.85\n",
      "Episode: 561/1000, steps: 858/1000, total steps: 184311, loss: -50.41, current reward: 246.10, running reward: 176.30\n",
      "Episode: 562/1000, steps: 182/1000, total steps: 184493, loss: -48.64, current reward: 267.93, running reward: 176.23\n",
      "Episode: 563/1000, steps: 191/1000, total steps: 184684, loss: -48.98, current reward: 266.80, running reward: 176.46\n",
      "Episode: 564/1000, steps: 170/1000, total steps: 184854, loss: -49.39, current reward: 280.52, running reward: 176.74\n",
      "Episode: 565/1000, steps: 193/1000, total steps: 185047, loss: -49.94, current reward: 275.08, running reward: 179.10\n",
      "Episode: 566/1000, steps: 93/1000, total steps: 185140, loss: -51.04, current reward: -23.44, running reward: 178.53\n",
      "Episode: 567/1000, steps: 270/1000, total steps: 185410, loss: -51.58, current reward: 251.25, running reward: 178.42\n",
      "Episode: 568/1000, steps: 268/1000, total steps: 185678, loss: -52.24, current reward: 157.13, running reward: 177.23\n",
      "Episode: 569/1000, steps: 1000/1000, total steps: 186678, loss: -49.86, current reward: 143.85, running reward: 178.21\n",
      "Episode: 570/1000, steps: 493/1000, total steps: 187171, loss: -46.56, current reward: 267.00, running reward: 178.35\n",
      "Episode: 571/1000, steps: 210/1000, total steps: 187381, loss: -46.58, current reward: 305.16, running reward: 178.54\n",
      "Episode: 572/1000, steps: 195/1000, total steps: 187576, loss: -46.92, current reward: 268.85, running reward: 178.48\n",
      "Episode: 573/1000, steps: 195/1000, total steps: 187771, loss: -47.38, current reward: 247.94, running reward: 178.06\n",
      "Episode: 574/1000, steps: 146/1000, total steps: 187917, loss: -48.02, current reward: 31.19, running reward: 175.57\n",
      "Episode: 575/1000, steps: 186/1000, total steps: 188103, loss: -48.90, current reward: 289.82, running reward: 175.82\n",
      "Episode: 576/1000, steps: 150/1000, total steps: 188253, loss: -48.61, current reward: 268.56, running reward: 175.74\n",
      "Episode: 577/1000, steps: 166/1000, total steps: 188419, loss: -49.63, current reward: 254.99, running reward: 175.24\n",
      "Episode: 578/1000, steps: 1000/1000, total steps: 189419, loss: -48.84, current reward: 182.00, running reward: 174.13\n",
      "Episode: 579/1000, steps: 1000/1000, total steps: 190419, loss: -47.60, current reward: 128.59, running reward: 172.96\n",
      "Episode: 580/1000, steps: 451/1000, total steps: 190870, loss: -48.95, current reward: 265.25, running reward: 175.53\n",
      "Episode: 581/1000, steps: 202/1000, total steps: 191072, loss: -50.79, current reward: 270.81, running reward: 177.84\n",
      "Episode: 582/1000, steps: 258/1000, total steps: 191330, loss: -52.79, current reward: 288.17, running reward: 178.18\n",
      "Episode: 583/1000, steps: 124/1000, total steps: 191454, loss: -54.41, current reward: 49.05, running reward: 177.43\n",
      "Episode: 584/1000, steps: 179/1000, total steps: 191633, loss: -56.35, current reward: 225.99, running reward: 177.08\n",
      "Episode: 585/1000, steps: 525/1000, total steps: 192158, loss: -58.80, current reward: 195.50, running reward: 176.35\n",
      "Episode: 586/1000, steps: 221/1000, total steps: 192379, loss: -59.44, current reward: 256.04, running reward: 178.73\n",
      "Episode: 587/1000, steps: 170/1000, total steps: 192549, loss: -59.19, current reward: 259.65, running reward: 181.20\n",
      "Episode: 588/1000, steps: 185/1000, total steps: 192734, loss: -58.43, current reward: 257.60, running reward: 180.91\n",
      "Episode: 589/1000, steps: 165/1000, total steps: 192899, loss: -57.59, current reward: 246.50, running reward: 183.78\n",
      "Episode: 590/1000, steps: 366/1000, total steps: 193265, loss: -56.25, current reward: 277.42, running reward: 186.88\n",
      "Episode: 591/1000, steps: 199/1000, total steps: 193464, loss: -55.33, current reward: 284.71, running reward: 187.09\n",
      "Episode: 592/1000, steps: 192/1000, total steps: 193656, loss: -55.06, current reward: 264.27, running reward: 187.08\n",
      "Episode: 593/1000, steps: 242/1000, total steps: 193898, loss: -54.46, current reward: 258.15, running reward: 189.44\n",
      "Episode: 594/1000, steps: 394/1000, total steps: 194292, loss: -53.79, current reward: 236.07, running reward: 191.30\n",
      "Episode: 595/1000, steps: 1000/1000, total steps: 195292, loss: -51.57, current reward: 164.71, running reward: 192.05\n",
      "Episode: 596/1000, steps: 286/1000, total steps: 195578, loss: -50.86, current reward: 282.47, running reward: 194.38\n",
      "Episode: 597/1000, steps: 148/1000, total steps: 195726, loss: -50.48, current reward: 287.45, running reward: 199.98\n",
      "Episode: 598/1000, steps: 89/1000, total steps: 195815, loss: -51.62, current reward: 41.20, running reward: 202.86\n",
      "########## Episode Solved ###########\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed=SEED)\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "env.seed(SEED)\n",
    "agent = DDPG(env, \"LunarLanderContinuous\", device=device)\n",
    "agent.learn(env, EPISODES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlXN0fz1Qeb1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DDPG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
