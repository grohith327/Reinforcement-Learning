{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 700\n",
    "BATCH_SIZE = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        obs = torch.tensor(obs, dtype=torch.float)\n",
    "        if done: reward = -10 ## Specific to Cartpole env\n",
    "        return obs, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Network - FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyFC(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay\n",
    "\n",
    "[Schaul et al.](https://arxiv.org/pdf/1511.05952.pdf) propose using binary heap for faster search and retreival of samples. However, In this implementation we use a list. <br />\n",
    "The samples are prioritized based on TD error. Instead of purely sampling only on TD error (greedy approach), an alpha parameter is used to interpolate between uniform random and greedy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedExperienceReplay:\n",
    "    \"\"\"\n",
    "    Implementation is adapted from: https://github.com/higgsfield/RL-Adventure \n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size=100000, alpha=0.6):\n",
    "        self.alpha = alpha ## Determines how much prioritization is used\n",
    "        self.state = []\n",
    "        self.action = []\n",
    "        self.next_state = []\n",
    "        self.reward = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.priorities = np.zeros((buffer_size,)).astype(np.float) + 1e-5 ## TD errors\n",
    "        self.count = 0\n",
    "    \n",
    "    def store(self, state, action, next_state, reward):\n",
    "        if(len(self.state) == self.buffer_size):\n",
    "            self.state = self.state[1:]\n",
    "            self.action = self.action[1:]\n",
    "            self.next_state = self.next_state[1:]\n",
    "            self.reward = self.reward[1:]\n",
    "        \n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.next_state.append(next_state)\n",
    "        self.reward.append(reward)\n",
    "        max_priority = self.priorities.max()\n",
    "        self.priorities[self.count] = max_priority\n",
    "        self.count += 1\n",
    "        self.count = min(self.count, self.buffer_size - 1)\n",
    "    \n",
    "    def sample_batch(self, batch_size, beta=0.4):\n",
    "        probs  = self.priorities[:self.count] ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        idxs = np.random.choice(len(self.state), batch_size, p=probs)\n",
    "        state = torch.stack(self.state)[idxs]\n",
    "        action = torch.tensor(self.action, dtype=torch.long)[idxs]\n",
    "        next_state = torch.stack(self.next_state)[idxs]\n",
    "        reward = torch.tensor(self.reward, dtype=torch.float)[idxs]\n",
    "\n",
    "        ## Beta parameter is used to anneal the amount of importance sampling\n",
    "        total = len(self.state)\n",
    "        weights  = (total * probs[idxs]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "        return (state, action, next_state, reward, idxs, weights)\n",
    "    \n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        for i, priority in zip(idxs, priorities):\n",
    "            self.priorities[i] = abs(priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, obs_size, action_size, device, gamma=0.99, lr=0.001):\n",
    "        self.target = PolicyFC(obs_size, action_size).to(device)\n",
    "        self.target.eval()\n",
    "        self.policy = PolicyFC(obs_size, action_size).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "        self.buffer = PrioritizedExperienceReplay()\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "    \n",
    "    def loss_fct(self, target, pred):\n",
    "        return F.smooth_l1_loss(pred, target, reduction=\"none\")\n",
    "    \n",
    "    def forward(self, policy, obs, grad=False):\n",
    "        obs = obs.to(self.device)\n",
    "        if(obs.size() == (4,)):\n",
    "            obs = obs.unsqueeze(0)\n",
    "        q_values = policy(obs)\n",
    "        if(not grad):\n",
    "            q_values = q_values.detach()\n",
    "        action = torch.argmax(q_values, 1)\n",
    "        return q_values, action\n",
    "    \n",
    "    def optimize_policy(self, batch, beta):\n",
    "        self.optimizer.zero_grad()\n",
    "        state, action, next_state, reward, idxs, weights = batch\n",
    "        weights = weights.to(device)\n",
    "        action = action.unsqueeze(1).to(device)\n",
    "        reward = reward.to(self.device)\n",
    "        Q, _ = self.forward(self.policy, state, grad=True)\n",
    "        _, next_action = self.forward(self.policy, next_state)\n",
    "        next_Q, _ = self.forward(self.target, next_state)\n",
    "        ## Target value estimation is made using both networks. Prevents overestimation\n",
    "        Q_target = next_Q.gather(1, next_action.unsqueeze(-1)).squeeze()\n",
    "        target = reward + self.gamma * Q_target\n",
    "        Q = Q.gather(1, action).squeeze()\n",
    "        loss = self.loss_fct(Q, target)\n",
    "        loss = loss * weights\n",
    "        priorities = loss + 1e-5\n",
    "        self.buffer.update_priorities(idxs, priorities.detach().cpu().numpy())\n",
    "        loss = loss.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.policy.eval()\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        torch.save(self.target.state_dict(), \"DQN_Agent.bin\")\n",
    "        self.policy.train()\n",
    "    \n",
    "    def load_policy(self, path=None):\n",
    "        if path is None:\n",
    "            path = \"DQN_Agent.bin\"\n",
    "        \n",
    "        self.target.load_state_dict(torch.load(path))\n",
    "        print(\"Successfully loaded\")\n",
    "    \n",
    "    def evaluate_policy(self, env):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while(not done):\n",
    "            obs = obs.unsqueeze(0)\n",
    "            obs = obs.to(self.device)\n",
    "            env.render()\n",
    "            with torch.no_grad():\n",
    "                q_values = self.target(obs)\n",
    "            action = torch.argmax(q_values, 1).item()\n",
    "            obs, reward, done = env.step(action)\n",
    "            print(f\"{count}, {action}, {reward}\")\n",
    "            count += 1\n",
    "    \n",
    "    def get_beta(self, curr_eps, total_eps):\n",
    "        \"\"\"\n",
    "        Reduce beta as episodes trained increases.\n",
    "        \"\"\"\n",
    "        beta_start = 0.4\n",
    "        beta = beta_start + curr_eps * (1.0 - beta_start) / total_eps\n",
    "        beta = min(1.0, beta)\n",
    "        return beta\n",
    "    \n",
    "    def get_eps(self, i, decay=100):\n",
    "        \"\"\"\n",
    "        Reduce epsilon as training progresses to reduce exlporation.\n",
    "        \"\"\"\n",
    "        epsilon_start = 1.0\n",
    "        epsilon_final = 0.05\n",
    "        eps = epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * i / decay)\n",
    "        return eps\n",
    "    \n",
    "    def learn(self, env, episodes, batch_size):\n",
    "        writer = SummaryWriter()\n",
    "        counter = 1\n",
    "        loss_count = 0\n",
    "        reward_count = 0\n",
    "        for eps in range(episodes):\n",
    "            obs = env.reset()\n",
    "            loss_tracker = AverageMeter()\n",
    "            reward_tracker = AverageMeter()\n",
    "            for t in range(1000):\n",
    "                epsilon = self.get_eps(eps)\n",
    "                if(np.random.rand() <= epsilon): ## Epsilon greedy\n",
    "                    action = np.random.randint(self.action_size)\n",
    "                else:\n",
    "                    _, action = self.forward(self.policy, obs)\n",
    "                    action = action.item()\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                self.buffer.store(obs, action, next_obs, reward)\n",
    "                reward_tracker.update(reward)\n",
    "\n",
    "                if(len(self.buffer) >= batch_size):\n",
    "                    batch = self.buffer.sample_batch(batch_size)\n",
    "                    beta = self.get_beta(eps, episodes)\n",
    "                    loss = self.optimize_policy(batch, beta)\n",
    "                    loss_tracker.update(loss)\n",
    "                    writer.add_scalar('Loss', loss, loss_count)\n",
    "                    loss_count += 1\n",
    "                    if(counter % 200 == 0): ## Delayed update of target. Promotes exploration\n",
    "                        self.update_target()\n",
    "                \n",
    "                if done: break\n",
    "\n",
    "                counter += 1\n",
    "                obs = next_obs\n",
    "            \n",
    "            writer.add_scalar(\"Reward\", reward_tracker.sum, reward_count)\n",
    "            reward_count += 1\n",
    "            \n",
    "            if((eps + 1) % 10 == 0):\n",
    "                print(f\"Episode: {eps}/{episodes}, step: {t+1}/1000, Epsilon: {epsilon}, reward: {reward_tracker.sum}, loss: {loss_tracker.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "env = PytorchWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(obs_size, action_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load tensorboard for visualization of loss\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(env, EPISODES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load_policy() # Load trained policy from local\n",
    "agent.evaluate_policy(env) # Evaluate target policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
